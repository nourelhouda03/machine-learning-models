{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_dl_models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmF8TSRK5t9W"
      },
      "source": [
        "preprocessing (cleanning the comments)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV1VH-j-2-f2",
        "outputId": "861a081d-cb40-4a23-ddc3-8b9f8e9e9a1d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install  alphabet_detector \n",
        "!pip install pyarabic\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/datasetfinal.xlsx\")\n",
        "\n",
        "from alphabet_detector import AlphabetDetector\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "\n",
        "#remove any othe language letters\n",
        "def keep_only_arabic(text):\n",
        "    ad = AlphabetDetector()\n",
        "    clean_lines = list()\n",
        "    for line in text.splitlines():\n",
        "        clean_line = list()\n",
        "        for word in line.split():\n",
        "            if len(word) > 1:\n",
        "                if ad.is_arabic(word):\n",
        "                    if word.isalpha():\n",
        "                        clean_line.append(word)\n",
        "        clean_lines.append(' '.join(clean_line))\n",
        "    return '\\n'.join(clean_lines)\n",
        "\n",
        "# remove empty lines\n",
        "def remove_empty_lines(text):\n",
        "    lines = [s.rstrip() for s in text.split(\"\\n\") if s.rstrip()]\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "import pyarabic.araby as araby\n",
        "def normalizeArabic(text):\n",
        "    text = text.strip()\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    text = re.sub(noise, '', text)\n",
        "    text = re.sub(r'(.)\\1+', r\"\\1\", text) # Remove longation\n",
        "    return araby.strip_tashkeel(text)\n",
        "    \n",
        "#delete everything except text \n",
        "def char_rmvl(sentence):\n",
        "  sen_split = sentence.split()\n",
        "  for idx1,word in enumerate(sen_split):       \n",
        "             sen_split[idx1] = re.sub('[^a-zا-ي]', '', word)       \n",
        "  sentence = ' '.join(sen_split)    \n",
        "  return sentence \n",
        "stops = set(stopwords.words(\"arabic\"))\n",
        "stop_word_comp = {\"،\",\"آض\",\"آمينَ\",\"آه\",\"آهاً\",\"آي\",\"أ\",\"أب\",\"أجل\",\"أجمع\",\"أخ\",\"أخذ\",\"أصبح\",\n",
        "                  \"أضحى\",\"أقبل\",\"أقل\",\"أكثر\",\"ألا\",\"أم\",\"أما\",\"أمامك\",\"أمامكَ\",\"أمسى\",\"أمّا\",\"أن\",\"أنا\",\"أنت\",\"\",\"أنتما\",\"أنتن\",\"أنتِ\",\n",
        "                  \"أنشأ\",\"أنّى\",\"أو\",\"أوشك\",\"أولئك\",\"أولئكم\",\"أولاء\",\"أولالك\",\"أوّهْ\",\"أي\",\"أيا\",\"أين\",\"أينما\",\"أيّ\",\"أَنَّ\",\"أََيُّ\",\"أُفٍّ\",\"إذ\",\"إذا\",\n",
        "                  \"إذاً\",\"إذما\",\"إذن\",\"إلى\",\"إليكم\",\"إليكما\",\"إليكنّ\",\"إليكَ\",\"إلَيْكَ\",\"إلّا\",\"إمّا\",\"إن\",\"إنّما\",\"إي\",\"إياك\",\"إياكم\",\"إياكما\",\"إياكن\",\n",
        "                  \"إيانا\",\"إياه\",\"إياها\",\"إياهم\",\"إياهما\",\"إياهن\",\"إياي\",\"إيهٍ\",\"إِنَّ\",\"ا\",\"ابتدأ\",\"اثر\",\"اجل\",\"احد\",\"اخرى\",\"اخلولق\",\"اذا\",\"اربعة\",\n",
        "                  \"ارتدّ\",\"استحال\",\"اطار\",\"اعادة\",\"اعلنت\",\"اف\",\"اكثر\",\"اكد\",\"الألاء\",\"الألى\",\"الا\",\"الاخيرة\",\"الان\",\"الاول\",\"الاولى\",\"التى\",\"التي\",\n",
        "                  \"الثاني\",\"الثانية\",\"الذاتي\",\"الذى\",\"الذي\",\"الذين\",\"السابق\",\"الف\",\"اللائي\",\"اللاتي\",\"اللتان\",\"اللتيا\",\"اللتين\",\"اللذان\",\"اللذين\",\"اللواتي\",\n",
        "                  \"الماضي\",\"المقبل\",\"الوقت\",\"الى\",\"اليوم\",\"اما\",\"امام\",\"امس\",\"ان\",\"انبرى\",\"انقلب\",\"انه\",\"انها\",\"او\",\"اول\",\"اي\",\"ايار\",\"ايام\",\n",
        "                  \"ايضا\",\"ب\",\"بات\",\"باسم\",\"بان\",\"بخٍ\",\"برس\",\"بسبب\",\"بسّ\",\"بشكل\",\"بضع\",\"بطآن\",\"بعد\",\"بعض\",\"بك\",\"بكم\",\"بكما\",\"بكن\",\n",
        "                  \"بل\",\"بلى\",\"بما\",\"بماذا\",\"بمن\",\"بن\",\"بنا\",\"به\",\"بها\",\"بي\",\"بيد\",\"بين\",\"بَسْ\",\"بَلْهَ\",\"بِئْسَ\",\"تانِ\",\"تانِك\",\"تبدّل\",\"تجاه\",\"تحوّل\",\n",
        "                  \"تلقاء\",\"تلك\",\"تلكم\",\"تلكما\",\"تم\",\"تينك\",\"تَيْنِ\",\"تِه\",\"تِي\",\"ثلاثة\",\"ثم\",\"ثمّ\",\"ثمّة\",\"ثُمَّ\",\"جعل\",\"جلل\",\"جميع\",\"جير\",\"حار\",\"حاشا\",\n",
        "                  \"حاليا\",\"حاي\",\"حتى\",\"حرى\",\"حسب\",\"حم\",\"حوالى\",\"حول\",\"حيث\",\"حيثما\",\"حين\",\"حيَّ\",\"حَبَّذَا\",\"حَتَّى\",\"حَذارِ\",\"خلا\",\"خلال\",\"دون\",\n",
        "                  \"دونك\",\"ذا\",\"ذات\",\"ذاك\",\"ذانك\",\"ذانِ\",\"ذلك\",\"ذلكم\",\"ذلكما\",\"ذلكن\",\"ذو\",\"ذوا\",\"ذواتا\",\"ذواتي\",\"ذيت\",\"ذينك\",\"ذَيْنِ\",\"ذِه\",\"ذِي\",\n",
        "                  \"راح\",\"رجع\",\"رويدك\",\"ريث\",\"رُبَّ\",\"زيارة\",\"سبحان\",\"سرعان\",\"سنة\",\"سنوات\",\"سوف\",\"سوى\",\"سَاءَ\",\"سَاءَمَا\",\"شبه\",\"شخصا\",\"شرع\",\n",
        "                  \"شَتَّانَ\",\"صار\",\"صباح\",\"صفر\",\"صهٍ\",\"صهْ\",\"ضد\",\"ضمن\",\"طاق\",\"طالما\",\"طفق\",\"طَق\",\"ظلّ\",\"عاد\",\"عام\",\"عاما\",\"عامة\",\"عدا\",\n",
        "                  \"عدة\",\"عدد\",\"عدم\",\"عسى\",\"عشر\",\"عشرة\",\"علق\",\"على\",\"عليك\",\"عليه\",\"عليها\",\"علًّ\",\"عن\",\"عند\",\"عندما\",\"عوض\",\"عين\",\"عَدَسْ\",\n",
        "                  \"عَمَّا\",\"غدا\",\"غير\",\"ـ\",\"ف\",\"فان\",\"فلان\",\"فو\",\"فى\",\"في\",\"فيم\",\"فيما\",\"فيه\",\"فيها\",\"قال\",\"قام\",\"قبل\",\"قد\",\"قطّ\",\"قلما\",\"قوة\",\"كأنّما\",\n",
        "                  \"كأين\",\"كأيّ\",\"كأيّن\",\"كاد\",\"كان\",\"كانت\",\"كذا\",\"كذلك\",\"كرب\",\"كل\",\"كلا\",\"كلاهما\",\"كلتا\",\"كلم\",\"كليكما\",\"كليهما\",\"كلّما\",\"كلَّا\",\"كم\",\n",
        "                  \"كما\",\"كي\",\"كيت\",\"كيف\",\"كيفما\",\"كَأَنَّ\",\"كِخ\",\"لئن\",\"لا\",\"لات\",\"لاسيما\",\"لدن\",\"لدى\",\"لعمر\",\"لقاء\",\"لك\",\"لكم\",\"لكما\",\"لكن\",\n",
        "                  \"لكنَّما\",\"لكي\",\"لكيلا\",\"للامم\",\"لم\",\"لما\",\"لمّا\",\"لن\",\"لنا\",\"له\",\"لها\",\"لو\",\"لوكالة\",\"لولا\",\"لوما\",\"لي\",\"لَسْتَ\",\"لَسْتُ\",\"لَسْتُم\",\"لَسْتُمَا\",\n",
        "                  \"لَسْتُنَّ\",\"لَسْتِ\",\"لَسْنَ\",\"لَعَلَّ\",\"لَكِنَّ\",\"لَيْتَ\",\"لَيْسَ\",\"لَيْسَا\",\"لَيْسَتَا\",\"لَيْسَتْ\",\"لَيْسُوا\",\"لَِسْنَا\",\"ما\",\"ماانفك\",\"مابرح\",\"مادام\",\"ماذا\",\"مازال\",\n",
        "                  \"مافتئ\",\"مايو\",\"متى\",\"مثل\",\"مذ\",\"مساء\",\"مع\",\"معاذ\",\"مقابل\",\"مكانكم\",\"مكانكما\",\"مكانكنّ\",\"مكانَك\",\"مليار\",\"مليون\",\"مما\",\"ممن\",\n",
        "                  \"من\",\"منذ\",\"منها\",\"مه\",\"مهما\",\"مَنْ\",\"مِن\",\"نحن\",\"نحو\",\"نعم\",\"نفس\",\"نفسه\",\"نهاية\",\"نَخْ\",\"نِعِمّا\",\"نِعْمَ\",\"ها\",\"هاؤم\",\"هاكَ\",\"هاهنا\",\n",
        "                  \"هبّ\",\"هذا\",\"هذه\",\"هكذا\",\"هل\",\"هلمَّ\",\"هلّا\",\"هم\",\"هما\",\"هن\",\"هنا\",\"هناك\",\"هنالك\",\"هو\",\"هي\",\"هيا\",\"هيت\",\"هيّا\",\"هَؤلاء\",\n",
        "                  \"هَاتانِ\",\"هَاتَيْنِ\",\"هَاتِه\",\"هَاتِي\",\"هَجْ\",\"هَذا\",\"هَذانِ\",\"هَذَيْنِ\",\"هَذِه\",\"هَذِي\",\"هَيْهَاتَ\",\"و\",\"و6\",\"وا\",\"واحد\",\"واضاف\",\"واضافت\",\n",
        "                  \"واكد\",\"وان\",\"واهاً\",\"واوضح\",\"وراءَك\",\"وفي\",\"وقال\",\"وقالت\",\"وقد\",\"وقف\",\"وكان\",\"وكانت\",\"ولا\",\"ولم\",\"ومن\",\"مَن\",\"وهو\",\n",
        "                  \"وهي\",\"ويكأنّ\",\"وَيْ\",\"وُشْكَانََ\",\"يكون\",\"يمكن\",\"يوم\",\"ّأيّان\",\"أنتم\" , 'إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', \n",
        "                  'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', \n",
        "                    'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى',\n",
        "                    'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن',\n",
        "                    'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا',\n",
        "                    'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', \n",
        "                    'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', \n",
        "                    'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا',\n",
        "                    'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', \n",
        "                    'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما',\n",
        "                    'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك',\n",
        "                    'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا', 'يمالى', 'قل', 'كثر', 'ألي', 'ليك', 'ليكم', \n",
        "                    'نتيا', 'نتوما', 'رانا' ,'بصح', 'هوما', 'وين', 'أمبعد', 'أومبعد', 'شوية', 'شويا', 'وش', 'واش', 'بوش', 'بواش','و'}\n",
        "def remove_stop_words(text):\n",
        "    zen = TextBlob(text)\n",
        "    words = zen.words\n",
        "    return \" \".join([w for w in words if not w in stops and not w in stop_word_comp and len(w) >= 2])\n",
        "\n",
        "\n",
        "def all_togther(text):\n",
        "  text = keep_only_arabic(text)\n",
        "  text = remove_empty_lines(text)\n",
        "  text = remove_emoji(text)\n",
        "  text = normalizeArabic(text)\n",
        "  text = char_rmvl(text)\n",
        "  #text = text.lower()\n",
        "  text = remove_stop_words(text)\n",
        "  text = ISRIStemmer().suf32(text)\n",
        "  return text\n",
        "\n",
        "\n",
        "df['comments'] = df['comments'].apply(lambda x:all_togther(x))\n",
        "nan_value = float(\"NaN\")\n",
        "df.replace(\"\", nan_value, inplace=True)\n",
        "df.dropna(subset = [\"comments\"], inplace=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: alphabet_detector in /usr/local/lib/python3.7/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.7/dist-packages (0.6.11)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNAJzAQt51H3"
      },
      "source": [
        "split data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0vo5T4O8LqR"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df['hate ']= df['hate '].map({'yes':1,'no':0 })\n",
        "\n",
        "df.sample(5)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['comments'], df['hate '], test_size=0.2, stratify=df['hate '])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "4JkK1_ZZB6mz",
        "outputId": "f0eb081c-34a3-4e2b-9097-c815ac02c35e"
      },
      "source": [
        "df.sample(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>comments</th>\n",
              "      <th>Topic</th>\n",
              "      <th>hate</th>\n",
              "      <th>cyberbullying التنمر الإلكتروني</th>\n",
              "      <th>Abusive and Offensive Language السب والكلام الـمسيئ</th>\n",
              "      <th>Source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1972</th>\n",
              "      <td>id1973</td>\n",
              "      <td>الهم اني صايم وزير فاشل مال راتبك قيو روح ترنك بف</td>\n",
              "      <td>price hikes</td>\n",
              "      <td>1</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>Facebook</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>id1457</td>\n",
              "      <td>يازلما اجهل منك ترا قط عيني واخبث منك تلد النس...</td>\n",
              "      <td>religion speech</td>\n",
              "      <td>1</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>YouTube</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5619</th>\n",
              "      <td>id5621</td>\n",
              "      <td>واله وجهك صحيح بوناني قوليها جد وهبي حبيبتك رو...</td>\n",
              "      <td>Misogyny</td>\n",
              "      <td>1</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>YouTube</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3490</th>\n",
              "      <td>id3491</td>\n",
              "      <td>قناه خاصه علاقه بالشعب الجزاري تمثل النظام الح...</td>\n",
              "      <td>Morocco politics</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>YouTube</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9004</th>\n",
              "      <td>id9006</td>\n",
              "      <td>المواطن خاصو الحياه الكريمه مشي العلاج الدعوه ...</td>\n",
              "      <td>president's speech</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>Facebook</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Id  ...    Source\n",
              "1972  id1973  ...  Facebook\n",
              "1456  id1457  ...   YouTube\n",
              "5619  id5621  ...   YouTube\n",
              "3490  id3491  ...   YouTube\n",
              "9004  id9006  ...  Facebook\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCdSyrxB54qb"
      },
      "source": [
        "tokanization and padding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmRORsVI-7Jg"
      },
      "source": [
        "\n",
        "# Import the tools needed from keras\n",
        "import keras.backend as K\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# Initialize and fit the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Use that tokenizer to transform the text messages in the training and test sets\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad the sequences so each sequence is the same length\n",
        "X_train_seq_padded = pad_sequences(X_train_seq, 200)\n",
        "X_test_seq_padded = pad_sequences(X_test_seq, 200)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL_ES7Ab6Emb"
      },
      "source": [
        "simple LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uI-FLeq--52",
        "outputId": "758233a8-8b4b-489c-88dd-6ed4038c571c"
      },
      "source": [
        "# Construct a simple RNN model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(len(tokenizer.index_word)+1, 32))\n",
        "model.add(LSTM(32, dropout=0, recurrent_dropout=0))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit the RNN model\n",
        "history = model.fit(X_train_seq_padded, y_train, \n",
        "                    batch_size=32, epochs=10,\n",
        "                    validation_data=(X_test_seq_padded, y_test))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, None, 32)          1084480   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,093,889\n",
            "Trainable params: 1,093,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "231/231 [==============================] - 29s 114ms/step - loss: 0.6799 - accuracy: 0.5611 - val_loss: 0.6320 - val_accuracy: 0.6368\n",
            "Epoch 2/10\n",
            "231/231 [==============================] - 26s 112ms/step - loss: 0.3121 - accuracy: 0.8767 - val_loss: 0.6939 - val_accuracy: 0.6531\n",
            "Epoch 3/10\n",
            "231/231 [==============================] - 26s 112ms/step - loss: 0.0674 - accuracy: 0.9849 - val_loss: 0.9676 - val_accuracy: 0.6460\n",
            "Epoch 4/10\n",
            "231/231 [==============================] - 26s 112ms/step - loss: 0.0329 - accuracy: 0.9924 - val_loss: 1.1370 - val_accuracy: 0.6450\n",
            "Epoch 5/10\n",
            "231/231 [==============================] - 26s 113ms/step - loss: 0.0185 - accuracy: 0.9960 - val_loss: 1.0611 - val_accuracy: 0.6417\n",
            "Epoch 6/10\n",
            "231/231 [==============================] - 26s 112ms/step - loss: 0.0162 - accuracy: 0.9966 - val_loss: 1.0338 - val_accuracy: 0.6515\n",
            "Epoch 7/10\n",
            "231/231 [==============================] - 26s 111ms/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 1.1184 - val_accuracy: 0.6471\n",
            "Epoch 8/10\n",
            "231/231 [==============================] - 26s 112ms/step - loss: 0.0126 - accuracy: 0.9969 - val_loss: 1.2248 - val_accuracy: 0.6477\n",
            "Epoch 9/10\n",
            "231/231 [==============================] - 26s 112ms/step - loss: 0.0230 - accuracy: 0.9943 - val_loss: 1.3195 - val_accuracy: 0.6319\n",
            "Epoch 10/10\n",
            "231/231 [==============================] - 26s 111ms/step - loss: 0.0176 - accuracy: 0.9934 - val_loss: 1.4165 - val_accuracy: 0.6308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55cURWWn-lHZ",
        "outputId": "f0c24ad2-64e5-45e7-feaa-b12c473ce255"
      },
      "source": [
        "\n",
        "# Generate generalization metrics\n",
        "score = model.evaluate(X_test_seq_padded, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.4165034294128418 / Test accuracy: 0.6308360695838928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoikTugu6J-P"
      },
      "source": [
        "simple GRU \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL_yqcbI_GLD",
        "outputId": "d502b362-85f3-4cb0-9690-fb7ed74ff478"
      },
      "source": [
        "from keras.layers import Dense, Embedding, LSTM, GRU\n",
        "\n",
        "# Construct a simple RNN model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(len(tokenizer.index_word)+1, 32))\n",
        "model.add(GRU(32, dropout=0, recurrent_dropout=0))\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit the RNN model\n",
        "history = model.fit(X_train_seq_padded, y_train, \n",
        "                    batch_size=32, epochs=10,\n",
        "                    validation_data=(X_test_seq_padded, y_test))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, None, 32)          1084480   \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 32)                6336      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,091,905\n",
            "Trainable params: 1,091,905\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "231/231 [==============================] - 28s 111ms/step - loss: 0.6798 - accuracy: 0.5640 - val_loss: 0.6142 - val_accuracy: 0.6553\n",
            "Epoch 2/10\n",
            "231/231 [==============================] - 25s 109ms/step - loss: 0.3145 - accuracy: 0.8685 - val_loss: 0.7794 - val_accuracy: 0.6526\n",
            "Epoch 3/10\n",
            "231/231 [==============================] - 25s 109ms/step - loss: 0.0576 - accuracy: 0.9830 - val_loss: 1.0358 - val_accuracy: 0.6536\n",
            "Epoch 4/10\n",
            "231/231 [==============================] - 25s 108ms/step - loss: 0.0212 - accuracy: 0.9960 - val_loss: 1.0661 - val_accuracy: 0.6379\n",
            "Epoch 5/10\n",
            "231/231 [==============================] - 25s 109ms/step - loss: 0.0174 - accuracy: 0.9961 - val_loss: 1.0849 - val_accuracy: 0.6308\n",
            "Epoch 6/10\n",
            "231/231 [==============================] - 25s 109ms/step - loss: 0.0168 - accuracy: 0.9964 - val_loss: 1.1721 - val_accuracy: 0.6336\n",
            "Epoch 7/10\n",
            "231/231 [==============================] - 25s 107ms/step - loss: 0.0148 - accuracy: 0.9953 - val_loss: 1.1012 - val_accuracy: 0.6450\n",
            "Epoch 8/10\n",
            "231/231 [==============================] - 25s 108ms/step - loss: 0.0122 - accuracy: 0.9968 - val_loss: 1.2628 - val_accuracy: 0.6455\n",
            "Epoch 9/10\n",
            "231/231 [==============================] - 25s 108ms/step - loss: 0.0191 - accuracy: 0.9946 - val_loss: 1.2068 - val_accuracy: 0.6450\n",
            "Epoch 10/10\n",
            "231/231 [==============================] - 25s 109ms/step - loss: 0.0121 - accuracy: 0.9964 - val_loss: 1.1316 - val_accuracy: 0.6422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lurjsXNJBt8p",
        "outputId": "ae4d318e-1662-4317-92e1-6dc6e5e3b18a"
      },
      "source": [
        "# Generate generalization metrics\n",
        "score = model.evaluate(X_test_seq_padded, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.1315873861312866 / Test accuracy: 0.6422367095947266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjG1h2lD6Oo2"
      },
      "source": [
        "Bi-LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-avCuUYa337F",
        "outputId": "b720e782-d875-4b0a-8081-8709b12b3cf3"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(tokenizer.index_word)+1, 16))\n",
        "\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, dropout=0, recurrent_dropout=0)))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit(X_train_seq_padded, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.20,\n",
        "                    verbose=1)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "185/185 [==============================] - 30s 136ms/step - loss: 0.6800 - acc: 0.5726 - val_loss: 0.7000 - val_acc: 0.5780\n",
            "Epoch 2/10\n",
            "185/185 [==============================] - 25s 133ms/step - loss: 0.3969 - acc: 0.8304 - val_loss: 0.6917 - val_acc: 0.6404\n",
            "Epoch 3/10\n",
            "185/185 [==============================] - 25s 134ms/step - loss: 0.0832 - acc: 0.9756 - val_loss: 1.0561 - val_acc: 0.6357\n",
            "Epoch 4/10\n",
            "185/185 [==============================] - 25s 133ms/step - loss: 0.0289 - acc: 0.9939 - val_loss: 1.1089 - val_acc: 0.6255\n",
            "Epoch 5/10\n",
            "185/185 [==============================] - 25s 135ms/step - loss: 0.0201 - acc: 0.9963 - val_loss: 1.1525 - val_acc: 0.6180\n",
            "Epoch 6/10\n",
            "185/185 [==============================] - 25s 134ms/step - loss: 0.0167 - acc: 0.9971 - val_loss: 1.2632 - val_acc: 0.6303\n",
            "Epoch 7/10\n",
            "185/185 [==============================] - 25s 133ms/step - loss: 0.0151 - acc: 0.9964 - val_loss: 1.3107 - val_acc: 0.6153\n",
            "Epoch 8/10\n",
            "185/185 [==============================] - 24s 132ms/step - loss: 0.0124 - acc: 0.9976 - val_loss: 1.2410 - val_acc: 0.6255\n",
            "Epoch 9/10\n",
            "185/185 [==============================] - 25s 133ms/step - loss: 0.0133 - acc: 0.9971 - val_loss: 1.2533 - val_acc: 0.6201\n",
            "Epoch 10/10\n",
            "185/185 [==============================] - 25s 135ms/step - loss: 0.0117 - acc: 0.9969 - val_loss: 1.3534 - val_acc: 0.6235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPXcXm5cBwxn",
        "outputId": "d594d1fd-6fbd-43ca-b404-007037850f71"
      },
      "source": [
        "# Generate generalization metrics\n",
        "score = model.evaluate(X_test_seq_padded, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.3517460823059082 / Test accuracy: 0.6254071593284607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgSt7s-eH-rz",
        "outputId": "53a17a12-a0a7-46d5-a364-6f426d8a97ff"
      },
      "source": [
        "model = Sequential()\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout, GRU\n",
        "\n",
        "model.add(Embedding(len(tokenizer.index_word)+1,64))\n",
        "\n",
        "\n",
        "model.add(LSTM(32))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "#model.summary()\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_seq_padded, y_train, \n",
        "                    batch_size=32, epochs=10,\n",
        "                    validation_data=(X_test_seq_padded, y_test))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "231/231 [==============================] - 31s 121ms/step - loss: 0.6807 - accuracy: 0.5590 - val_loss: 0.6099 - val_accuracy: 0.6640\n",
            "Epoch 2/10\n",
            "231/231 [==============================] - 28s 120ms/step - loss: 0.3494 - accuracy: 0.8583 - val_loss: 0.6977 - val_accuracy: 0.6553\n",
            "Epoch 3/10\n",
            "231/231 [==============================] - 27s 118ms/step - loss: 0.0732 - accuracy: 0.9773 - val_loss: 0.9175 - val_accuracy: 0.6466\n",
            "Epoch 4/10\n",
            "231/231 [==============================] - 27s 116ms/step - loss: 0.0358 - accuracy: 0.9924 - val_loss: 1.1703 - val_accuracy: 0.6455\n",
            "Epoch 5/10\n",
            "231/231 [==============================] - 27s 118ms/step - loss: 0.0202 - accuracy: 0.9960 - val_loss: 1.3322 - val_accuracy: 0.6466\n",
            "Epoch 6/10\n",
            "231/231 [==============================] - 27s 116ms/step - loss: 0.0150 - accuracy: 0.9955 - val_loss: 1.3923 - val_accuracy: 0.6477\n",
            "Epoch 7/10\n",
            "231/231 [==============================] - 27s 116ms/step - loss: 0.0103 - accuracy: 0.9955 - val_loss: 1.3389 - val_accuracy: 0.6384\n",
            "Epoch 8/10\n",
            "231/231 [==============================] - 27s 117ms/step - loss: 0.0141 - accuracy: 0.9933 - val_loss: 1.9507 - val_accuracy: 0.6406\n",
            "Epoch 9/10\n",
            "231/231 [==============================] - 27s 116ms/step - loss: 0.0056 - accuracy: 0.9966 - val_loss: 2.0670 - val_accuracy: 0.6450\n",
            "Epoch 10/10\n",
            "231/231 [==============================] - 27s 116ms/step - loss: 0.0065 - accuracy: 0.9980 - val_loss: 2.0130 - val_accuracy: 0.6412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BcnLTIEKAK4",
        "outputId": "ea969de5-de13-4551-ab1b-fecb6da37b76"
      },
      "source": [
        "# Generate generalization metrics\n",
        "score = model.evaluate(X_test_seq_padded, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 2.0129759311676025 / Test accuracy: 0.641150951385498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Pm12pg1KDpQ",
        "outputId": "f82b6f12-5cd0-47fa-9cac-cfe6e1c5a329"
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(len(tokenizer.index_word)+1, 32))\n",
        "\n",
        "model.add(Conv1D(40,5))\n",
        "model.add(MaxPooling1D(2,2))\n",
        "model.add(Dense(10,activation=\"relu\"))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_seq_padded, y_train, \n",
        "                    batch_size=32, epochs=10,\n",
        "                    validation_data=(X_test_seq_padded, y_test))\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "231/231 [==============================] - 8s 29ms/step - loss: 0.6896 - accuracy: 0.5364 - val_loss: 0.6870 - val_accuracy: 0.5604\n",
            "Epoch 2/10\n",
            "231/231 [==============================] - 6s 28ms/step - loss: 0.6834 - accuracy: 0.5715 - val_loss: 0.6866 - val_accuracy: 0.5620\n",
            "Epoch 3/10\n",
            "231/231 [==============================] - 7s 29ms/step - loss: 0.6729 - accuracy: 0.5740 - val_loss: 0.6915 - val_accuracy: 0.5613\n",
            "Epoch 4/10\n",
            "231/231 [==============================] - 8s 34ms/step - loss: 0.6618 - accuracy: 0.5817 - val_loss: 0.7041 - val_accuracy: 0.5614\n",
            "Epoch 5/10\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.6540 - accuracy: 0.5796 - val_loss: 0.7136 - val_accuracy: 0.5612\n",
            "Epoch 6/10\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.6461 - accuracy: 0.5827 - val_loss: 0.7254 - val_accuracy: 0.5611\n",
            "Epoch 7/10\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.6408 - accuracy: 0.5924 - val_loss: 0.7346 - val_accuracy: 0.5605\n",
            "Epoch 8/10\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.6393 - accuracy: 0.5867 - val_loss: 0.7459 - val_accuracy: 0.5605\n",
            "Epoch 9/10\n",
            "231/231 [==============================] - 8s 34ms/step - loss: 0.6368 - accuracy: 0.5956 - val_loss: 0.7520 - val_accuracy: 0.5607\n",
            "Epoch 10/10\n",
            "231/231 [==============================] - 8s 33ms/step - loss: 0.6395 - accuracy: 0.5826 - val_loss: 0.7623 - val_accuracy: 0.5604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9olQZaZPV_L",
        "outputId": "d19a3e3e-b4fc-482a-b84b-aa8dbca8d6f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Generate generalization metrics\n",
        "score = model.evaluate(X_test_seq_padded, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.7622650861740112 / Test accuracy: 0.5604324340820312\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}