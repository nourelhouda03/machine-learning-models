{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fastext+DL_models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMLmTKpOBGSw",
        "outputId": "b0399627-7e8e-4bb4-c8c7-ae14ce5adb86"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install  alphabet_detector \n",
        "!pip install pyarabic\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pandas as pd\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/datasetfinal.xlsx\")\n",
        "\n",
        "from alphabet_detector import AlphabetDetector\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "\n",
        "#remove any othe language letters\n",
        "def keep_only_arabic(text):\n",
        "    ad = AlphabetDetector()\n",
        "    clean_lines = list()\n",
        "    for line in text.splitlines():\n",
        "        clean_line = list()\n",
        "        for word in line.split():\n",
        "            if len(word) > 1:\n",
        "                if ad.is_arabic(word):\n",
        "                    if word.isalpha():\n",
        "                        clean_line.append(word)\n",
        "        clean_lines.append(' '.join(clean_line))\n",
        "    return '\\n'.join(clean_lines)\n",
        "\n",
        "# remove empty lines\n",
        "def remove_empty_lines(text):\n",
        "    lines = [s.rstrip() for s in text.split(\"\\n\") if s.rstrip()]\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "import pyarabic.araby as araby\n",
        "def normalizeArabic(text):\n",
        "    text = text.strip()\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    text = re.sub(noise, '', text)\n",
        "    text = re.sub(r'(.)\\1+', r\"\\1\", text) # Remove longation\n",
        "    return araby.strip_tashkeel(text)\n",
        "    \n",
        "#delete everything except text \n",
        "def char_rmvl(sentence):\n",
        "  sen_split = sentence.split()\n",
        "  for idx1,word in enumerate(sen_split):       \n",
        "             sen_split[idx1] = re.sub('[^a-zا-ي]', '', word)       \n",
        "  sentence = ' '.join(sen_split)    \n",
        "  return sentence \n",
        "stops = set(stopwords.words(\"arabic\"))\n",
        "stop_word_comp = {\"،\",\"آض\",\"آمينَ\",\"آه\",\"آهاً\",\"آي\",\"أ\",\"أب\",\"أجل\",\"أجمع\",\"أخ\",\"أخذ\",\"أصبح\",\n",
        "                  \"أضحى\",\"أقبل\",\"أقل\",\"أكثر\",\"ألا\",\"أم\",\"أما\",\"أمامك\",\"أمامكَ\",\"أمسى\",\"أمّا\",\"أن\",\"أنا\",\"أنت\",\"\",\"أنتما\",\"أنتن\",\"أنتِ\",\n",
        "                  \"أنشأ\",\"أنّى\",\"أو\",\"أوشك\",\"أولئك\",\"أولئكم\",\"أولاء\",\"أولالك\",\"أوّهْ\",\"أي\",\"أيا\",\"أين\",\"أينما\",\"أيّ\",\"أَنَّ\",\"أََيُّ\",\"أُفٍّ\",\"إذ\",\"إذا\",\n",
        "                  \"إذاً\",\"إذما\",\"إذن\",\"إلى\",\"إليكم\",\"إليكما\",\"إليكنّ\",\"إليكَ\",\"إلَيْكَ\",\"إلّا\",\"إمّا\",\"إن\",\"إنّما\",\"إي\",\"إياك\",\"إياكم\",\"إياكما\",\"إياكن\",\n",
        "                  \"إيانا\",\"إياه\",\"إياها\",\"إياهم\",\"إياهما\",\"إياهن\",\"إياي\",\"إيهٍ\",\"إِنَّ\",\"ا\",\"ابتدأ\",\"اثر\",\"اجل\",\"احد\",\"اخرى\",\"اخلولق\",\"اذا\",\"اربعة\",\n",
        "                  \"ارتدّ\",\"استحال\",\"اطار\",\"اعادة\",\"اعلنت\",\"اف\",\"اكثر\",\"اكد\",\"الألاء\",\"الألى\",\"الا\",\"الاخيرة\",\"الان\",\"الاول\",\"الاولى\",\"التى\",\"التي\",\n",
        "                  \"الثاني\",\"الثانية\",\"الذاتي\",\"الذى\",\"الذي\",\"الذين\",\"السابق\",\"الف\",\"اللائي\",\"اللاتي\",\"اللتان\",\"اللتيا\",\"اللتين\",\"اللذان\",\"اللذين\",\"اللواتي\",\n",
        "                  \"الماضي\",\"المقبل\",\"الوقت\",\"الى\",\"اليوم\",\"اما\",\"امام\",\"امس\",\"ان\",\"انبرى\",\"انقلب\",\"انه\",\"انها\",\"او\",\"اول\",\"اي\",\"ايار\",\"ايام\",\n",
        "                  \"ايضا\",\"ب\",\"بات\",\"باسم\",\"بان\",\"بخٍ\",\"برس\",\"بسبب\",\"بسّ\",\"بشكل\",\"بضع\",\"بطآن\",\"بعد\",\"بعض\",\"بك\",\"بكم\",\"بكما\",\"بكن\",\n",
        "                  \"بل\",\"بلى\",\"بما\",\"بماذا\",\"بمن\",\"بن\",\"بنا\",\"به\",\"بها\",\"بي\",\"بيد\",\"بين\",\"بَسْ\",\"بَلْهَ\",\"بِئْسَ\",\"تانِ\",\"تانِك\",\"تبدّل\",\"تجاه\",\"تحوّل\",\n",
        "                  \"تلقاء\",\"تلك\",\"تلكم\",\"تلكما\",\"تم\",\"تينك\",\"تَيْنِ\",\"تِه\",\"تِي\",\"ثلاثة\",\"ثم\",\"ثمّ\",\"ثمّة\",\"ثُمَّ\",\"جعل\",\"جلل\",\"جميع\",\"جير\",\"حار\",\"حاشا\",\n",
        "                  \"حاليا\",\"حاي\",\"حتى\",\"حرى\",\"حسب\",\"حم\",\"حوالى\",\"حول\",\"حيث\",\"حيثما\",\"حين\",\"حيَّ\",\"حَبَّذَا\",\"حَتَّى\",\"حَذارِ\",\"خلا\",\"خلال\",\"دون\",\n",
        "                  \"دونك\",\"ذا\",\"ذات\",\"ذاك\",\"ذانك\",\"ذانِ\",\"ذلك\",\"ذلكم\",\"ذلكما\",\"ذلكن\",\"ذو\",\"ذوا\",\"ذواتا\",\"ذواتي\",\"ذيت\",\"ذينك\",\"ذَيْنِ\",\"ذِه\",\"ذِي\",\n",
        "                  \"راح\",\"رجع\",\"رويدك\",\"ريث\",\"رُبَّ\",\"زيارة\",\"سبحان\",\"سرعان\",\"سنة\",\"سنوات\",\"سوف\",\"سوى\",\"سَاءَ\",\"سَاءَمَا\",\"شبه\",\"شخصا\",\"شرع\",\n",
        "                  \"شَتَّانَ\",\"صار\",\"صباح\",\"صفر\",\"صهٍ\",\"صهْ\",\"ضد\",\"ضمن\",\"طاق\",\"طالما\",\"طفق\",\"طَق\",\"ظلّ\",\"عاد\",\"عام\",\"عاما\",\"عامة\",\"عدا\",\n",
        "                  \"عدة\",\"عدد\",\"عدم\",\"عسى\",\"عشر\",\"عشرة\",\"علق\",\"على\",\"عليك\",\"عليه\",\"عليها\",\"علًّ\",\"عن\",\"عند\",\"عندما\",\"عوض\",\"عين\",\"عَدَسْ\",\n",
        "                  \"عَمَّا\",\"غدا\",\"غير\",\"ـ\",\"ف\",\"فان\",\"فلان\",\"فو\",\"فى\",\"في\",\"فيم\",\"فيما\",\"فيه\",\"فيها\",\"قال\",\"قام\",\"قبل\",\"قد\",\"قطّ\",\"قلما\",\"قوة\",\"كأنّما\",\n",
        "                  \"كأين\",\"كأيّ\",\"كأيّن\",\"كاد\",\"كان\",\"كانت\",\"كذا\",\"كذلك\",\"كرب\",\"كل\",\"كلا\",\"كلاهما\",\"كلتا\",\"كلم\",\"كليكما\",\"كليهما\",\"كلّما\",\"كلَّا\",\"كم\",\n",
        "                  \"كما\",\"كي\",\"كيت\",\"كيف\",\"كيفما\",\"كَأَنَّ\",\"كِخ\",\"لئن\",\"لا\",\"لات\",\"لاسيما\",\"لدن\",\"لدى\",\"لعمر\",\"لقاء\",\"لك\",\"لكم\",\"لكما\",\"لكن\",\n",
        "                  \"لكنَّما\",\"لكي\",\"لكيلا\",\"للامم\",\"لم\",\"لما\",\"لمّا\",\"لن\",\"لنا\",\"له\",\"لها\",\"لو\",\"لوكالة\",\"لولا\",\"لوما\",\"لي\",\"لَسْتَ\",\"لَسْتُ\",\"لَسْتُم\",\"لَسْتُمَا\",\n",
        "                  \"لَسْتُنَّ\",\"لَسْتِ\",\"لَسْنَ\",\"لَعَلَّ\",\"لَكِنَّ\",\"لَيْتَ\",\"لَيْسَ\",\"لَيْسَا\",\"لَيْسَتَا\",\"لَيْسَتْ\",\"لَيْسُوا\",\"لَِسْنَا\",\"ما\",\"ماانفك\",\"مابرح\",\"مادام\",\"ماذا\",\"مازال\",\n",
        "                  \"مافتئ\",\"مايو\",\"متى\",\"مثل\",\"مذ\",\"مساء\",\"مع\",\"معاذ\",\"مقابل\",\"مكانكم\",\"مكانكما\",\"مكانكنّ\",\"مكانَك\",\"مليار\",\"مليون\",\"مما\",\"ممن\",\n",
        "                  \"من\",\"منذ\",\"منها\",\"مه\",\"مهما\",\"مَنْ\",\"مِن\",\"نحن\",\"نحو\",\"نعم\",\"نفس\",\"نفسه\",\"نهاية\",\"نَخْ\",\"نِعِمّا\",\"نِعْمَ\",\"ها\",\"هاؤم\",\"هاكَ\",\"هاهنا\",\n",
        "                  \"هبّ\",\"هذا\",\"هذه\",\"هكذا\",\"هل\",\"هلمَّ\",\"هلّا\",\"هم\",\"هما\",\"هن\",\"هنا\",\"هناك\",\"هنالك\",\"هو\",\"هي\",\"هيا\",\"هيت\",\"هيّا\",\"هَؤلاء\",\n",
        "                  \"هَاتانِ\",\"هَاتَيْنِ\",\"هَاتِه\",\"هَاتِي\",\"هَجْ\",\"هَذا\",\"هَذانِ\",\"هَذَيْنِ\",\"هَذِه\",\"هَذِي\",\"هَيْهَاتَ\",\"و\",\"و6\",\"وا\",\"واحد\",\"واضاف\",\"واضافت\",\n",
        "                  \"واكد\",\"وان\",\"واهاً\",\"واوضح\",\"وراءَك\",\"وفي\",\"وقال\",\"وقالت\",\"وقد\",\"وقف\",\"وكان\",\"وكانت\",\"ولا\",\"ولم\",\"ومن\",\"مَن\",\"وهو\",\n",
        "                  \"وهي\",\"ويكأنّ\",\"وَيْ\",\"وُشْكَانََ\",\"يكون\",\"يمكن\",\"يوم\",\"ّأيّان\",\"أنتم\" , 'إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', \n",
        "                  'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', \n",
        "                    'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى',\n",
        "                    'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن',\n",
        "                    'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا',\n",
        "                    'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', \n",
        "                    'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', \n",
        "                    'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا',\n",
        "                    'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', \n",
        "                    'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما',\n",
        "                    'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك',\n",
        "                    'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا', 'يمالى', 'قل', 'كثر', 'ألي', 'ليك', 'ليكم', \n",
        "                    'نتيا', 'نتوما', 'رانا' ,'بصح', 'هوما', 'وين', 'أمبعد', 'أومبعد', 'شوية', 'شويا', 'وش', 'واش', 'بوش', 'بواش','و'}\n",
        "def remove_stop_words(text):\n",
        "    zen = TextBlob(text)\n",
        "    words = zen.words\n",
        "    return \" \".join([w for w in words if not w in stops and not w in stop_word_comp and len(w) >= 2])\n",
        "\n",
        "\n",
        "def all_togther(text):\n",
        "  text = keep_only_arabic(text)\n",
        "  text = remove_empty_lines(text)\n",
        "  text = remove_emoji(text)\n",
        "  text = normalizeArabic(text)\n",
        "  text = char_rmvl(text)\n",
        "  text = text.lower()\n",
        "  text = remove_stop_words(text)\n",
        "  text = ISRIStemmer().suf32(text)\n",
        "  return text\n",
        "\n",
        "\n",
        "df['comments'] = df['comments'].apply(lambda x:all_togther(x))\n",
        "nan_value = float(\"NaN\")\n",
        "df.replace(\"\", nan_value, inplace=True)\n",
        "df.dropna(subset = [\"comments\"], inplace=True)\n",
        "#print(remove_emoji('🌏.حتي الكويت عطلة رسميه 🌷 المولد 👑❤👑🤲🤲🇲🇦🇲🇦🇲🇦🇲🇦النبوي ✌ ماذا⌨↪•⏱⏳⏰⏲#⃣ ⏪ 📺يقصد 💩صطيف ♥️🤲  ⁦فركاشه?Fakio👑👑+🇮🇱🇮🇱🐪🐪🐪🐪🐪🐪🐪👉')) #we didn't catch some emoji\n",
        "#print(normalizeArabic('مَاذَااَتَخِذُ مِنٛ اَجٛرَررررراءَتٛ؟')) #done \n",
        "#print(char_rmvl('مَاذَااَتَخِذُ05550 #$%&()*+,،-./:;<=>؟?@[\\]^_`{|}مِنٛ اَجٛرَررررراءَتٛ  👑❤👑🤲🤲🇲🇦🇲🇦🇲🇦🇲🇦النبوي ✌ ماذا⌨↪•⏱⏳⏰⏲#⃣ ⏪ 📺؟')) #done \n",
        "#print(remove_stop_words('دير حل رانا كرهنا من الوعود الكاذبة والهدرة بلاش')) #done \n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: alphabet_detector in /usr/local/lib/python3.7/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.7/dist-packages (0.6.14)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pyarabic) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1wwBimpU8T9"
      },
      "source": [
        "downloading Fastext "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCZYYfkbCRTA",
        "outputId": "6dcf25a9-879a-4db3-9533-8ab1306ccfab"
      },
      "source": [
        "\n",
        "#Download FastText Embeddings (Vectors not the model)\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ar.300.vec.gz\n",
        "!gzip -q cc.ar.300.vec.gz -d .\n",
        "\n",
        "#%%\n",
        "\n",
        "#Loading FastText Embeddings.\n",
        "from tqdm import tqdm\n",
        "import codecs\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"Loading FastText Embeddings\"\"\"\n",
        "print(\"Loading FastText Embeddings...\")\n",
        "FastText = {}\n",
        "f = codecs.open('/content/cc.ar.300.vec', encoding='utf-8')\n",
        "for line in tqdm(f):\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    FastText[word] = coefs\n",
        "f.close()\n",
        "\n",
        "#%%\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-08 15:47:36--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ar.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1272365870 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.ar.300.vec.gz’\n",
            "\n",
            "cc.ar.300.vec.gz    100%[===================>]   1.18G  21.8MB/s    in 56s     \n",
            "\n",
            "2021-08-08 15:48:34 (21.5 MB/s) - ‘cc.ar.300.vec.gz’ saved [1272365870/1272365870]\n",
            "\n",
            "gzip: cc.ar.300.vec already exists; do you wish to overwrite (y or n)? y\n",
            "y\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "934it [00:00, 9333.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading FastText Embeddings...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000001it [03:46, 8843.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4P-Ke9kVCq2"
      },
      "source": [
        "fasttext_embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsuISwnSFRIS",
        "outputId": "8427cad8-e5e3-4d1d-ad55-9efea019136d"
      },
      "source": [
        "\n",
        "#Tokenization and Sentence Encoding\n",
        "\n",
        "X_train_1 = df['comments']\n",
        "\n",
        "#Input_ids and Attention masks.\n",
        "X_train = []\n",
        "\n",
        "#Creating Embedding Matrix for fasttext embeddings\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing import text\n",
        "tokenizer = text.Tokenizer(num_words=30000)\n",
        "tokenizer.fit_on_texts(X_train_1)\n",
        "\n",
        "train_texts = tokenizer.texts_to_sequences(X_train_1)\n",
        "X_train_ft = sequence.pad_sequences(train_texts, maxlen=128)\n",
        "\n",
        "print(\"Dictionary: \"+str(len(tokenizer.word_index)))\n",
        "embedding_matrix = np.zeros((30000, 300))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index > 30000 - 1:\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = FastText.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary: 39137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipK_NL4CVHHj"
      },
      "source": [
        "spliting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwhuHHa-GoBC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = df['hate '].map({'yes':1,'no':0 })\n",
        "##Splitting fastText encoded data for training DL algorithms based on Bi-LSTMs and Bi-GRUs.\n",
        "## train data ==> (X_train_ft, y_train)\n",
        "## Validation data ==> (X_val_ft, y_val)\n",
        "## Test data ==> (X_test_ft, y_test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_ft,y)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6yfNgohVJZb"
      },
      "source": [
        "fasttext with LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqH5UU9rHiHL",
        "outputId": "8b0e3097-ddd0-4640-c073-e2401f03552a"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "model = Sequential()\n",
        "## embeddings layer (fasttext embedding 300 dimension size)\n",
        "model.add(Embedding(30000, 300, input_length=128, weights=[embedding_matrix], trainable=True))\n",
        "\n",
        "model.add(LSTM(units=128))\n",
        "#model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(X_train, y_train, validation_split=0.3, epochs=4)\n",
        "\n",
        "y_pred = (model.predict(X_test) >= 0.5)\n",
        "\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (None, 128, 300)          9000000   \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 128)               219648    \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 9,219,777\n",
            "Trainable params: 9,219,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "152/152 [==============================] - 65s 412ms/step - loss: 0.6485 - acc: 0.6307 - val_loss: 0.6112 - val_acc: 0.6684\n",
            "Epoch 2/4\n",
            "152/152 [==============================] - 62s 409ms/step - loss: 0.3427 - acc: 0.8566 - val_loss: 0.7921 - val_acc: 0.6646\n",
            "Epoch 3/4\n",
            "152/152 [==============================] - 62s 410ms/step - loss: 0.1109 - acc: 0.9694 - val_loss: 0.9019 - val_acc: 0.6279\n",
            "Epoch 4/4\n",
            "152/152 [==============================] - 62s 409ms/step - loss: 0.0397 - acc: 0.9907 - val_loss: 1.1900 - val_acc: 0.6414\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.60      0.61      1036\n",
            "           1       0.68      0.69      0.68      1266\n",
            "\n",
            "    accuracy                           0.65      2302\n",
            "   macro avg       0.65      0.65      0.65      2302\n",
            "weighted avg       0.65      0.65      0.65      2302\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls2v8kj7VO-P"
      },
      "source": [
        "fasttext with GRU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn-7s_1qTvdD",
        "outputId": "940b735c-3061-415c-ad39-50ab3bdc5fcd"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "model = Sequential()\n",
        "## embeddings layer (fasttext embedding 300 dimension size)\n",
        "model.add(Embedding(30000, 300, input_length=128, weights=[embedding_matrix], trainable=True))\n",
        "\n",
        "model.add(GRU(units=128))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(X_train, y_train, validation_split=0.3, epochs=4)\n",
        "\n",
        "y_pred = (model.predict(X_test) >= 0.5)\n",
        "\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_15 (Embedding)     (None, 128, 300)          9000000   \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 128)               165120    \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 9,165,249\n",
            "Trainable params: 9,165,249\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "152/152 [==============================] - 55s 353ms/step - loss: 0.6491 - acc: 0.6098 - val_loss: 0.6192 - val_acc: 0.6583\n",
            "Epoch 2/4\n",
            "152/152 [==============================] - 47s 311ms/step - loss: 0.3388 - acc: 0.8628 - val_loss: 0.7513 - val_acc: 0.6641\n",
            "Epoch 3/4\n",
            "152/152 [==============================] - 48s 313ms/step - loss: 0.0689 - acc: 0.9789 - val_loss: 1.1611 - val_acc: 0.6602\n",
            "Epoch 4/4\n",
            "152/152 [==============================] - 48s 314ms/step - loss: 0.0290 - acc: 0.9952 - val_loss: 1.3031 - val_acc: 0.6472\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.64      0.63      1036\n",
            "           1       0.69      0.67      0.68      1266\n",
            "\n",
            "    accuracy                           0.66      2302\n",
            "   macro avg       0.65      0.66      0.65      2302\n",
            "weighted avg       0.66      0.66      0.66      2302\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0DVbL6IVSWD"
      },
      "source": [
        "fasttext with Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR298hw6OkI_",
        "outputId": "a67be8a9-3b80-4693-ff16-3931b1e23606"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.__version__\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import warnings\n",
        "\n",
        "model = Sequential()\n",
        "model.add(tf.keras.layers.Embedding(30000, 300, input_length=128, weights=[embedding_matrix], trainable=True))\n",
        "\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(X_train, y_train, validation_split=0.3, epochs=4)\n",
        "\n",
        "y_pred = (model.predict(X_test) >= 0.5)\n",
        "\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     (None, 128, 300)          9000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 256)               439296    \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 9,439,553\n",
            "Trainable params: 9,439,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "152/152 [==============================] - 109s 695ms/step - loss: 0.6592 - binary_accuracy: 0.6063 - val_loss: 0.6057 - val_binary_accuracy: 0.6684\n",
            "Epoch 2/4\n",
            "152/152 [==============================] - 106s 695ms/step - loss: 0.3557 - binary_accuracy: 0.8546 - val_loss: 0.6842 - val_binary_accuracy: 0.6549\n",
            "Epoch 3/4\n",
            "152/152 [==============================] - 107s 701ms/step - loss: 0.0852 - binary_accuracy: 0.9743 - val_loss: 0.9174 - val_binary_accuracy: 0.6511\n",
            "Epoch 4/4\n",
            "152/152 [==============================] - 107s 702ms/step - loss: 0.0295 - binary_accuracy: 0.9942 - val_loss: 1.1894 - val_binary_accuracy: 0.6472\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.57      0.60      1036\n",
            "           1       0.67      0.73      0.70      1266\n",
            "\n",
            "    accuracy                           0.66      2302\n",
            "   macro avg       0.65      0.65      0.65      2302\n",
            "weighted avg       0.65      0.66      0.65      2302\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_iQvwVtVXXt"
      },
      "source": [
        "CNN with fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "id": "n5uovtlAONs-",
        "outputId": "2f92cd2f-022d-464f-b4fd-bf713c77d106"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "model = Sequential()\n",
        "## embeddings layer (fasttext embedding 300 dimension size)\n",
        "model.add(Embedding(30000, 300, input_length=128, weights=[embedding_matrix], trainable=True))\n",
        "model.add(Conv1D(activation='relu', filters=4, kernel_size=4))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(MaxPool1D())\n",
        "\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(X_train, y_train, validation_split=0.3, epochs=4)\n",
        "\n",
        "y_pred = (model.predict(X_test) >= 0.5)\n",
        "\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_17 (Embedding)     (None, 128, 300)          9000000   \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 125, 4)            4804      \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 125, 4)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 62, 4)             0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 62, 1)             5         \n",
            "=================================================================\n",
            "Total params: 9,004,809\n",
            "Trainable params: 9,004,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "152/152 [==============================] - 21s 131ms/step - loss: 0.6871 - acc: 0.5585 - val_loss: 0.6852 - val_acc: 0.5631\n",
            "Epoch 2/4\n",
            "152/152 [==============================] - 19s 128ms/step - loss: 0.6800 - acc: 0.5710 - val_loss: 0.6828 - val_acc: 0.5664\n",
            "Epoch 3/4\n",
            "152/152 [==============================] - 19s 126ms/step - loss: 0.6667 - acc: 0.5810 - val_loss: 0.6879 - val_acc: 0.5654\n",
            "Epoch 4/4\n",
            "152/152 [==============================] - 19s 127ms/step - loss: 0.6561 - acc: 0.5859 - val_loss: 0.6894 - val_acc: 0.5650\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-705e56522898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and unknown targets"
          ]
        }
      ]
    }
  ]
}