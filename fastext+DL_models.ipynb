{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fastext+DL_models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMLmTKpOBGSw",
        "outputId": "b0399627-7e8e-4bb4-c8c7-ae14ce5adb86"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install  alphabet_detector \n",
        "!pip install pyarabic\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pandas as pd\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/datasetfinal.xlsx\")\n",
        "\n",
        "from alphabet_detector import AlphabetDetector\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "\n",
        "#remove any othe language letters\n",
        "def keep_only_arabic(text):\n",
        "    ad = AlphabetDetector()\n",
        "    clean_lines = list()\n",
        "    for line in text.splitlines():\n",
        "        clean_line = list()\n",
        "        for word in line.split():\n",
        "            if len(word) > 1:\n",
        "                if ad.is_arabic(word):\n",
        "                    if word.isalpha():\n",
        "                        clean_line.append(word)\n",
        "        clean_lines.append(' '.join(clean_line))\n",
        "    return '\\n'.join(clean_lines)\n",
        "\n",
        "# remove empty lines\n",
        "def remove_empty_lines(text):\n",
        "    lines = [s.rstrip() for s in text.split(\"\\n\") if s.rstrip()]\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "import pyarabic.araby as araby\n",
        "def normalizeArabic(text):\n",
        "    text = text.strip()\n",
        "    text = re.sub(\"[Ø¥Ø£Ù±Ø¢Ø§]\", \"Ø§\", text)\n",
        "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
        "    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
        "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
        "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
        "    noise = re.compile(\"\"\" Ù‘    | # Tashdid\n",
        "                             Ù    | # Fatha\n",
        "                             Ù‹    | # Tanwin Fath\n",
        "                             Ù    | # Damma\n",
        "                             ÙŒ    | # Tanwin Damm\n",
        "                             Ù    | # Kasra\n",
        "                             Ù    | # Tanwin Kasr\n",
        "                             Ù’    | # Sukun\n",
        "                             Ù€     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    text = re.sub(noise, '', text)\n",
        "    text = re.sub(r'(.)\\1+', r\"\\1\", text) # Remove longation\n",
        "    return araby.strip_tashkeel(text)\n",
        "    \n",
        "#delete everything except text \n",
        "def char_rmvl(sentence):\n",
        "  sen_split = sentence.split()\n",
        "  for idx1,word in enumerate(sen_split):       \n",
        "             sen_split[idx1] = re.sub('[^a-zØ§-ÙŠ]', '', word)       \n",
        "  sentence = ' '.join(sen_split)    \n",
        "  return sentence \n",
        "stops = set(stopwords.words(\"arabic\"))\n",
        "stop_word_comp = {\"ØŒ\",\"Ø¢Ø¶\",\"Ø¢Ù…ÙŠÙ†Ù\",\"Ø¢Ù‡\",\"Ø¢Ù‡Ø§Ù‹\",\"Ø¢ÙŠ\",\"Ø£\",\"Ø£Ø¨\",\"Ø£Ø¬Ù„\",\"Ø£Ø¬Ù…Ø¹\",\"Ø£Ø®\",\"Ø£Ø®Ø°\",\"Ø£ØµØ¨Ø­\",\n",
        "                  \"Ø£Ø¶Ø­Ù‰\",\"Ø£Ù‚Ø¨Ù„\",\"Ø£Ù‚Ù„\",\"Ø£ÙƒØ«Ø±\",\"Ø£Ù„Ø§\",\"Ø£Ù…\",\"Ø£Ù…Ø§\",\"Ø£Ù…Ø§Ù…Ùƒ\",\"Ø£Ù…Ø§Ù…ÙƒÙ\",\"Ø£Ù…Ø³Ù‰\",\"Ø£Ù…Ù‘Ø§\",\"Ø£Ù†\",\"Ø£Ù†Ø§\",\"Ø£Ù†Øª\",\"\",\"Ø£Ù†ØªÙ…Ø§\",\"Ø£Ù†ØªÙ†\",\"Ø£Ù†ØªÙ\",\n",
        "                  \"Ø£Ù†Ø´Ø£\",\"Ø£Ù†Ù‘Ù‰\",\"Ø£Ùˆ\",\"Ø£ÙˆØ´Ùƒ\",\"Ø£ÙˆÙ„Ø¦Ùƒ\",\"Ø£ÙˆÙ„Ø¦ÙƒÙ…\",\"Ø£ÙˆÙ„Ø§Ø¡\",\"Ø£ÙˆÙ„Ø§Ù„Ùƒ\",\"Ø£ÙˆÙ‘Ù‡Ù’\",\"Ø£ÙŠ\",\"Ø£ÙŠØ§\",\"Ø£ÙŠÙ†\",\"Ø£ÙŠÙ†Ù…Ø§\",\"Ø£ÙŠÙ‘\",\"Ø£ÙÙ†Ù‘Ù\",\"Ø£ÙÙÙŠÙ‘Ù\",\"Ø£ÙÙÙ‘Ù\",\"Ø¥Ø°\",\"Ø¥Ø°Ø§\",\n",
        "                  \"Ø¥Ø°Ø§Ù‹\",\"Ø¥Ø°Ù…Ø§\",\"Ø¥Ø°Ù†\",\"Ø¥Ù„Ù‰\",\"Ø¥Ù„ÙŠÙƒÙ…\",\"Ø¥Ù„ÙŠÙƒÙ…Ø§\",\"Ø¥Ù„ÙŠÙƒÙ†Ù‘\",\"Ø¥Ù„ÙŠÙƒÙ\",\"Ø¥Ù„ÙÙŠÙ’ÙƒÙ\",\"Ø¥Ù„Ù‘Ø§\",\"Ø¥Ù…Ù‘Ø§\",\"Ø¥Ù†\",\"Ø¥Ù†Ù‘Ù…Ø§\",\"Ø¥ÙŠ\",\"Ø¥ÙŠØ§Ùƒ\",\"Ø¥ÙŠØ§ÙƒÙ…\",\"Ø¥ÙŠØ§ÙƒÙ…Ø§\",\"Ø¥ÙŠØ§ÙƒÙ†\",\n",
        "                  \"Ø¥ÙŠØ§Ù†Ø§\",\"Ø¥ÙŠØ§Ù‡\",\"Ø¥ÙŠØ§Ù‡Ø§\",\"Ø¥ÙŠØ§Ù‡Ù…\",\"Ø¥ÙŠØ§Ù‡Ù…Ø§\",\"Ø¥ÙŠØ§Ù‡Ù†\",\"Ø¥ÙŠØ§ÙŠ\",\"Ø¥ÙŠÙ‡Ù\",\"Ø¥ÙÙ†Ù‘Ù\",\"Ø§\",\"Ø§Ø¨ØªØ¯Ø£\",\"Ø§Ø«Ø±\",\"Ø§Ø¬Ù„\",\"Ø§Ø­Ø¯\",\"Ø§Ø®Ø±Ù‰\",\"Ø§Ø®Ù„ÙˆÙ„Ù‚\",\"Ø§Ø°Ø§\",\"Ø§Ø±Ø¨Ø¹Ø©\",\n",
        "                  \"Ø§Ø±ØªØ¯Ù‘\",\"Ø§Ø³ØªØ­Ø§Ù„\",\"Ø§Ø·Ø§Ø±\",\"Ø§Ø¹Ø§Ø¯Ø©\",\"Ø§Ø¹Ù„Ù†Øª\",\"Ø§Ù\",\"Ø§ÙƒØ«Ø±\",\"Ø§ÙƒØ¯\",\"Ø§Ù„Ø£Ù„Ø§Ø¡\",\"Ø§Ù„Ø£Ù„Ù‰\",\"Ø§Ù„Ø§\",\"Ø§Ù„Ø§Ø®ÙŠØ±Ø©\",\"Ø§Ù„Ø§Ù†\",\"Ø§Ù„Ø§ÙˆÙ„\",\"Ø§Ù„Ø§ÙˆÙ„Ù‰\",\"Ø§Ù„ØªÙ‰\",\"Ø§Ù„ØªÙŠ\",\n",
        "                  \"Ø§Ù„Ø«Ø§Ù†ÙŠ\",\"Ø§Ù„Ø«Ø§Ù†ÙŠØ©\",\"Ø§Ù„Ø°Ø§ØªÙŠ\",\"Ø§Ù„Ø°Ù‰\",\"Ø§Ù„Ø°ÙŠ\",\"Ø§Ù„Ø°ÙŠÙ†\",\"Ø§Ù„Ø³Ø§Ø¨Ù‚\",\"Ø§Ù„Ù\",\"Ø§Ù„Ù„Ø§Ø¦ÙŠ\",\"Ø§Ù„Ù„Ø§ØªÙŠ\",\"Ø§Ù„Ù„ØªØ§Ù†\",\"Ø§Ù„Ù„ØªÙŠØ§\",\"Ø§Ù„Ù„ØªÙŠÙ†\",\"Ø§Ù„Ù„Ø°Ø§Ù†\",\"Ø§Ù„Ù„Ø°ÙŠÙ†\",\"Ø§Ù„Ù„ÙˆØ§ØªÙŠ\",\n",
        "                  \"Ø§Ù„Ù…Ø§Ø¶ÙŠ\",\"Ø§Ù„Ù…Ù‚Ø¨Ù„\",\"Ø§Ù„ÙˆÙ‚Øª\",\"Ø§Ù„Ù‰\",\"Ø§Ù„ÙŠÙˆÙ…\",\"Ø§Ù…Ø§\",\"Ø§Ù…Ø§Ù…\",\"Ø§Ù…Ø³\",\"Ø§Ù†\",\"Ø§Ù†Ø¨Ø±Ù‰\",\"Ø§Ù†Ù‚Ù„Ø¨\",\"Ø§Ù†Ù‡\",\"Ø§Ù†Ù‡Ø§\",\"Ø§Ùˆ\",\"Ø§ÙˆÙ„\",\"Ø§ÙŠ\",\"Ø§ÙŠØ§Ø±\",\"Ø§ÙŠØ§Ù…\",\n",
        "                  \"Ø§ÙŠØ¶Ø§\",\"Ø¨\",\"Ø¨Ø§Øª\",\"Ø¨Ø§Ø³Ù…\",\"Ø¨Ø§Ù†\",\"Ø¨Ø®Ù\",\"Ø¨Ø±Ø³\",\"Ø¨Ø³Ø¨Ø¨\",\"Ø¨Ø³Ù‘\",\"Ø¨Ø´ÙƒÙ„\",\"Ø¨Ø¶Ø¹\",\"Ø¨Ø·Ø¢Ù†\",\"Ø¨Ø¹Ø¯\",\"Ø¨Ø¹Ø¶\",\"Ø¨Ùƒ\",\"Ø¨ÙƒÙ…\",\"Ø¨ÙƒÙ…Ø§\",\"Ø¨ÙƒÙ†\",\n",
        "                  \"Ø¨Ù„\",\"Ø¨Ù„Ù‰\",\"Ø¨Ù…Ø§\",\"Ø¨Ù…Ø§Ø°Ø§\",\"Ø¨Ù…Ù†\",\"Ø¨Ù†\",\"Ø¨Ù†Ø§\",\"Ø¨Ù‡\",\"Ø¨Ù‡Ø§\",\"Ø¨ÙŠ\",\"Ø¨ÙŠØ¯\",\"Ø¨ÙŠÙ†\",\"Ø¨ÙØ³Ù’\",\"Ø¨ÙÙ„Ù’Ù‡Ù\",\"Ø¨ÙØ¦Ù’Ø³Ù\",\"ØªØ§Ù†Ù\",\"ØªØ§Ù†ÙÙƒ\",\"ØªØ¨Ø¯Ù‘Ù„\",\"ØªØ¬Ø§Ù‡\",\"ØªØ­ÙˆÙ‘Ù„\",\n",
        "                  \"ØªÙ„Ù‚Ø§Ø¡\",\"ØªÙ„Ùƒ\",\"ØªÙ„ÙƒÙ…\",\"ØªÙ„ÙƒÙ…Ø§\",\"ØªÙ…\",\"ØªÙŠÙ†Ùƒ\",\"ØªÙÙŠÙ’Ù†Ù\",\"ØªÙÙ‡\",\"ØªÙÙŠ\",\"Ø«Ù„Ø§Ø«Ø©\",\"Ø«Ù…\",\"Ø«Ù…Ù‘\",\"Ø«Ù…Ù‘Ø©\",\"Ø«ÙÙ…Ù‘Ù\",\"Ø¬Ø¹Ù„\",\"Ø¬Ù„Ù„\",\"Ø¬Ù…ÙŠØ¹\",\"Ø¬ÙŠØ±\",\"Ø­Ø§Ø±\",\"Ø­Ø§Ø´Ø§\",\n",
        "                  \"Ø­Ø§Ù„ÙŠØ§\",\"Ø­Ø§ÙŠ\",\"Ø­ØªÙ‰\",\"Ø­Ø±Ù‰\",\"Ø­Ø³Ø¨\",\"Ø­Ù…\",\"Ø­ÙˆØ§Ù„Ù‰\",\"Ø­ÙˆÙ„\",\"Ø­ÙŠØ«\",\"Ø­ÙŠØ«Ù…Ø§\",\"Ø­ÙŠÙ†\",\"Ø­ÙŠÙ‘Ù\",\"Ø­ÙØ¨Ù‘ÙØ°ÙØ§\",\"Ø­ÙØªÙ‘ÙÙ‰\",\"Ø­ÙØ°Ø§Ø±Ù\",\"Ø®Ù„Ø§\",\"Ø®Ù„Ø§Ù„\",\"Ø¯ÙˆÙ†\",\n",
        "                  \"Ø¯ÙˆÙ†Ùƒ\",\"Ø°Ø§\",\"Ø°Ø§Øª\",\"Ø°Ø§Ùƒ\",\"Ø°Ø§Ù†Ùƒ\",\"Ø°Ø§Ù†Ù\",\"Ø°Ù„Ùƒ\",\"Ø°Ù„ÙƒÙ…\",\"Ø°Ù„ÙƒÙ…Ø§\",\"Ø°Ù„ÙƒÙ†\",\"Ø°Ùˆ\",\"Ø°ÙˆØ§\",\"Ø°ÙˆØ§ØªØ§\",\"Ø°ÙˆØ§ØªÙŠ\",\"Ø°ÙŠØª\",\"Ø°ÙŠÙ†Ùƒ\",\"Ø°ÙÙŠÙ’Ù†Ù\",\"Ø°ÙÙ‡\",\"Ø°ÙÙŠ\",\n",
        "                  \"Ø±Ø§Ø­\",\"Ø±Ø¬Ø¹\",\"Ø±ÙˆÙŠØ¯Ùƒ\",\"Ø±ÙŠØ«\",\"Ø±ÙØ¨Ù‘Ù\",\"Ø²ÙŠØ§Ø±Ø©\",\"Ø³Ø¨Ø­Ø§Ù†\",\"Ø³Ø±Ø¹Ø§Ù†\",\"Ø³Ù†Ø©\",\"Ø³Ù†ÙˆØ§Øª\",\"Ø³ÙˆÙ\",\"Ø³ÙˆÙ‰\",\"Ø³ÙØ§Ø¡Ù\",\"Ø³ÙØ§Ø¡ÙÙ…ÙØ§\",\"Ø´Ø¨Ù‡\",\"Ø´Ø®ØµØ§\",\"Ø´Ø±Ø¹\",\n",
        "                  \"Ø´ÙØªÙ‘ÙØ§Ù†Ù\",\"ØµØ§Ø±\",\"ØµØ¨Ø§Ø­\",\"ØµÙØ±\",\"ØµÙ‡Ù\",\"ØµÙ‡Ù’\",\"Ø¶Ø¯\",\"Ø¶Ù…Ù†\",\"Ø·Ø§Ù‚\",\"Ø·Ø§Ù„Ù…Ø§\",\"Ø·ÙÙ‚\",\"Ø·ÙÙ‚\",\"Ø¸Ù„Ù‘\",\"Ø¹Ø§Ø¯\",\"Ø¹Ø§Ù…\",\"Ø¹Ø§Ù…Ø§\",\"Ø¹Ø§Ù…Ø©\",\"Ø¹Ø¯Ø§\",\n",
        "                  \"Ø¹Ø¯Ø©\",\"Ø¹Ø¯Ø¯\",\"Ø¹Ø¯Ù…\",\"Ø¹Ø³Ù‰\",\"Ø¹Ø´Ø±\",\"Ø¹Ø´Ø±Ø©\",\"Ø¹Ù„Ù‚\",\"Ø¹Ù„Ù‰\",\"Ø¹Ù„ÙŠÙƒ\",\"Ø¹Ù„ÙŠÙ‡\",\"Ø¹Ù„ÙŠÙ‡Ø§\",\"Ø¹Ù„Ù‘Ù‹\",\"Ø¹Ù†\",\"Ø¹Ù†Ø¯\",\"Ø¹Ù†Ø¯Ù…Ø§\",\"Ø¹ÙˆØ¶\",\"Ø¹ÙŠÙ†\",\"Ø¹ÙØ¯ÙØ³Ù’\",\n",
        "                  \"Ø¹ÙÙ…Ù‘ÙØ§\",\"ØºØ¯Ø§\",\"ØºÙŠØ±\",\"Ù€\",\"Ù\",\"ÙØ§Ù†\",\"ÙÙ„Ø§Ù†\",\"ÙÙˆ\",\"ÙÙ‰\",\"ÙÙŠ\",\"ÙÙŠÙ…\",\"ÙÙŠÙ…Ø§\",\"ÙÙŠÙ‡\",\"ÙÙŠÙ‡Ø§\",\"Ù‚Ø§Ù„\",\"Ù‚Ø§Ù…\",\"Ù‚Ø¨Ù„\",\"Ù‚Ø¯\",\"Ù‚Ø·Ù‘\",\"Ù‚Ù„Ù…Ø§\",\"Ù‚ÙˆØ©\",\"ÙƒØ£Ù†Ù‘Ù…Ø§\",\n",
        "                  \"ÙƒØ£ÙŠÙ†\",\"ÙƒØ£ÙŠÙ‘\",\"ÙƒØ£ÙŠÙ‘Ù†\",\"ÙƒØ§Ø¯\",\"ÙƒØ§Ù†\",\"ÙƒØ§Ù†Øª\",\"ÙƒØ°Ø§\",\"ÙƒØ°Ù„Ùƒ\",\"ÙƒØ±Ø¨\",\"ÙƒÙ„\",\"ÙƒÙ„Ø§\",\"ÙƒÙ„Ø§Ù‡Ù…Ø§\",\"ÙƒÙ„ØªØ§\",\"ÙƒÙ„Ù…\",\"ÙƒÙ„ÙŠÙƒÙ…Ø§\",\"ÙƒÙ„ÙŠÙ‡Ù…Ø§\",\"ÙƒÙ„Ù‘Ù…Ø§\",\"ÙƒÙ„Ù‘ÙØ§\",\"ÙƒÙ…\",\n",
        "                  \"ÙƒÙ…Ø§\",\"ÙƒÙŠ\",\"ÙƒÙŠØª\",\"ÙƒÙŠÙ\",\"ÙƒÙŠÙÙ…Ø§\",\"ÙƒÙØ£ÙÙ†Ù‘Ù\",\"ÙƒÙØ®\",\"Ù„Ø¦Ù†\",\"Ù„Ø§\",\"Ù„Ø§Øª\",\"Ù„Ø§Ø³ÙŠÙ…Ø§\",\"Ù„Ø¯Ù†\",\"Ù„Ø¯Ù‰\",\"Ù„Ø¹Ù…Ø±\",\"Ù„Ù‚Ø§Ø¡\",\"Ù„Ùƒ\",\"Ù„ÙƒÙ…\",\"Ù„ÙƒÙ…Ø§\",\"Ù„ÙƒÙ†\",\n",
        "                  \"Ù„ÙƒÙ†Ù‘ÙÙ…Ø§\",\"Ù„ÙƒÙŠ\",\"Ù„ÙƒÙŠÙ„Ø§\",\"Ù„Ù„Ø§Ù…Ù…\",\"Ù„Ù…\",\"Ù„Ù…Ø§\",\"Ù„Ù…Ù‘Ø§\",\"Ù„Ù†\",\"Ù„Ù†Ø§\",\"Ù„Ù‡\",\"Ù„Ù‡Ø§\",\"Ù„Ùˆ\",\"Ù„ÙˆÙƒØ§Ù„Ø©\",\"Ù„ÙˆÙ„Ø§\",\"Ù„ÙˆÙ…Ø§\",\"Ù„ÙŠ\",\"Ù„ÙØ³Ù’ØªÙ\",\"Ù„ÙØ³Ù’ØªÙ\",\"Ù„ÙØ³Ù’ØªÙÙ…\",\"Ù„ÙØ³Ù’ØªÙÙ…ÙØ§\",\n",
        "                  \"Ù„ÙØ³Ù’ØªÙÙ†Ù‘Ù\",\"Ù„ÙØ³Ù’ØªÙ\",\"Ù„ÙØ³Ù’Ù†Ù\",\"Ù„ÙØ¹ÙÙ„Ù‘Ù\",\"Ù„ÙÙƒÙÙ†Ù‘Ù\",\"Ù„ÙÙŠÙ’ØªÙ\",\"Ù„ÙÙŠÙ’Ø³Ù\",\"Ù„ÙÙŠÙ’Ø³ÙØ§\",\"Ù„ÙÙŠÙ’Ø³ÙØªÙØ§\",\"Ù„ÙÙŠÙ’Ø³ÙØªÙ’\",\"Ù„ÙÙŠÙ’Ø³ÙÙˆØ§\",\"Ù„ÙÙØ³Ù’Ù†ÙØ§\",\"Ù…Ø§\",\"Ù…Ø§Ø§Ù†ÙÙƒ\",\"Ù…Ø§Ø¨Ø±Ø­\",\"Ù…Ø§Ø¯Ø§Ù…\",\"Ù…Ø§Ø°Ø§\",\"Ù…Ø§Ø²Ø§Ù„\",\n",
        "                  \"Ù…Ø§ÙØªØ¦\",\"Ù…Ø§ÙŠÙˆ\",\"Ù…ØªÙ‰\",\"Ù…Ø«Ù„\",\"Ù…Ø°\",\"Ù…Ø³Ø§Ø¡\",\"Ù…Ø¹\",\"Ù…Ø¹Ø§Ø°\",\"Ù…Ù‚Ø§Ø¨Ù„\",\"Ù…ÙƒØ§Ù†ÙƒÙ…\",\"Ù…ÙƒØ§Ù†ÙƒÙ…Ø§\",\"Ù…ÙƒØ§Ù†ÙƒÙ†Ù‘\",\"Ù…ÙƒØ§Ù†ÙÙƒ\",\"Ù…Ù„ÙŠØ§Ø±\",\"Ù…Ù„ÙŠÙˆÙ†\",\"Ù…Ù…Ø§\",\"Ù…Ù…Ù†\",\n",
        "                  \"Ù…Ù†\",\"Ù…Ù†Ø°\",\"Ù…Ù†Ù‡Ø§\",\"Ù…Ù‡\",\"Ù…Ù‡Ù…Ø§\",\"Ù…ÙÙ†Ù’\",\"Ù…ÙÙ†\",\"Ù†Ø­Ù†\",\"Ù†Ø­Ùˆ\",\"Ù†Ø¹Ù…\",\"Ù†ÙØ³\",\"Ù†ÙØ³Ù‡\",\"Ù†Ù‡Ø§ÙŠØ©\",\"Ù†ÙØ®Ù’\",\"Ù†ÙØ¹ÙÙ…Ù‘Ø§\",\"Ù†ÙØ¹Ù’Ù…Ù\",\"Ù‡Ø§\",\"Ù‡Ø§Ø¤Ù…\",\"Ù‡Ø§ÙƒÙ\",\"Ù‡Ø§Ù‡Ù†Ø§\",\n",
        "                  \"Ù‡Ø¨Ù‘\",\"Ù‡Ø°Ø§\",\"Ù‡Ø°Ù‡\",\"Ù‡ÙƒØ°Ø§\",\"Ù‡Ù„\",\"Ù‡Ù„Ù…Ù‘Ù\",\"Ù‡Ù„Ù‘Ø§\",\"Ù‡Ù…\",\"Ù‡Ù…Ø§\",\"Ù‡Ù†\",\"Ù‡Ù†Ø§\",\"Ù‡Ù†Ø§Ùƒ\",\"Ù‡Ù†Ø§Ù„Ùƒ\",\"Ù‡Ùˆ\",\"Ù‡ÙŠ\",\"Ù‡ÙŠØ§\",\"Ù‡ÙŠØª\",\"Ù‡ÙŠÙ‘Ø§\",\"Ù‡ÙØ¤Ù„Ø§Ø¡\",\n",
        "                  \"Ù‡ÙØ§ØªØ§Ù†Ù\",\"Ù‡ÙØ§ØªÙÙŠÙ’Ù†Ù\",\"Ù‡ÙØ§ØªÙÙ‡\",\"Ù‡ÙØ§ØªÙÙŠ\",\"Ù‡ÙØ¬Ù’\",\"Ù‡ÙØ°Ø§\",\"Ù‡ÙØ°Ø§Ù†Ù\",\"Ù‡ÙØ°ÙÙŠÙ’Ù†Ù\",\"Ù‡ÙØ°ÙÙ‡\",\"Ù‡ÙØ°ÙÙŠ\",\"Ù‡ÙÙŠÙ’Ù‡ÙØ§ØªÙ\",\"Ùˆ\",\"Ùˆ6\",\"ÙˆØ§\",\"ÙˆØ§Ø­Ø¯\",\"ÙˆØ§Ø¶Ø§Ù\",\"ÙˆØ§Ø¶Ø§ÙØª\",\n",
        "                  \"ÙˆØ§ÙƒØ¯\",\"ÙˆØ§Ù†\",\"ÙˆØ§Ù‡Ø§Ù‹\",\"ÙˆØ§ÙˆØ¶Ø­\",\"ÙˆØ±Ø§Ø¡ÙÙƒ\",\"ÙˆÙÙŠ\",\"ÙˆÙ‚Ø§Ù„\",\"ÙˆÙ‚Ø§Ù„Øª\",\"ÙˆÙ‚Ø¯\",\"ÙˆÙ‚Ù\",\"ÙˆÙƒØ§Ù†\",\"ÙˆÙƒØ§Ù†Øª\",\"ÙˆÙ„Ø§\",\"ÙˆÙ„Ù…\",\"ÙˆÙ…Ù†\",\"Ù…ÙÙ†\",\"ÙˆÙ‡Ùˆ\",\n",
        "                  \"ÙˆÙ‡ÙŠ\",\"ÙˆÙŠÙƒØ£Ù†Ù‘\",\"ÙˆÙÙŠÙ’\",\"ÙˆÙØ´Ù’ÙƒÙØ§Ù†ÙÙ\",\"ÙŠÙƒÙˆÙ†\",\"ÙŠÙ…ÙƒÙ†\",\"ÙŠÙˆÙ…\",\"Ù‘Ø£ÙŠÙ‘Ø§Ù†\",\"Ø£Ù†ØªÙ…\" , 'Ø¥Ø°', 'Ø¥Ø°Ø§', 'Ø¥Ø°Ù…Ø§', 'Ø¥Ø°Ù†', 'Ø£Ù', 'Ø£Ù‚Ù„', 'Ø£ÙƒØ«Ø±', 'Ø£Ù„Ø§', 'Ø¥Ù„Ø§', 'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ø°ÙŠ', \n",
        "                  'Ø§Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ù„Ø§ØªÙŠ', 'Ø§Ù„Ù„Ø§Ø¦ÙŠ', 'Ø§Ù„Ù„ØªØ§Ù†', 'Ø§Ù„Ù„ØªÙŠØ§', 'Ø§Ù„Ù„ØªÙŠÙ†', 'Ø§Ù„Ù„Ø°Ø§Ù†', 'Ø§Ù„Ù„Ø°ÙŠÙ†', \n",
        "                    'Ø§Ù„Ù„ÙˆØ§ØªÙŠ', 'Ø¥Ù„Ù‰', 'Ø¥Ù„ÙŠÙƒ', 'Ø¥Ù„ÙŠÙƒÙ…', 'Ø¥Ù„ÙŠÙƒÙ…Ø§', 'Ø¥Ù„ÙŠÙƒÙ†', 'Ø£Ù…', 'Ø£Ù…Ø§', 'Ø£Ù…Ø§', 'Ø¥Ù…Ø§', 'Ø£Ù†', 'Ø¥Ù†', 'Ø¥Ù†Ø§', 'Ø£Ù†Ø§', 'Ø£Ù†Øª', 'Ø£Ù†ØªÙ…', 'Ø£Ù†ØªÙ…Ø§', 'Ø£Ù†ØªÙ†', 'Ø¥Ù†Ù…Ø§', 'Ø¥Ù†Ù‡', 'Ø£Ù†Ù‰', 'Ø£Ù†Ù‰',\n",
        "                    'Ø¢Ù‡', 'Ø¢Ù‡Ø§', 'Ø£Ùˆ', 'Ø£ÙˆÙ„Ø§Ø¡', 'Ø£ÙˆÙ„Ø¦Ùƒ', 'Ø£ÙˆÙ‡', 'Ø¢ÙŠ', 'Ø£ÙŠ', 'Ø£ÙŠÙ‡Ø§', 'Ø¥ÙŠ', 'Ø£ÙŠÙ†', 'Ø£ÙŠÙ†', 'Ø£ÙŠÙ†Ù…Ø§', 'Ø¥ÙŠÙ‡', 'Ø¨Ø®', 'Ø¨Ø³', 'Ø¨Ø¹Ø¯', 'Ø¨Ø¹Ø¶', 'Ø¨Ùƒ', 'Ø¨ÙƒÙ…', 'Ø¨ÙƒÙ…', 'Ø¨ÙƒÙ…Ø§', 'Ø¨ÙƒÙ†',\n",
        "                    'Ø¨Ù„', 'Ø¨Ù„Ù‰', 'Ø¨Ù…Ø§', 'Ø¨Ù…Ø§Ø°Ø§', 'Ø¨Ù…Ù†', 'Ø¨Ù†Ø§', 'Ø¨Ù‡', 'Ø¨Ù‡Ø§', 'Ø¨Ù‡Ù…', 'Ø¨Ù‡Ù…Ø§', 'Ø¨Ù‡Ù†', 'Ø¨ÙŠ', 'Ø¨ÙŠÙ†', 'Ø¨ÙŠØ¯', 'ØªÙ„Ùƒ', 'ØªÙ„ÙƒÙ…', 'ØªÙ„ÙƒÙ…Ø§', 'ØªÙ‡', 'ØªÙŠ', 'ØªÙŠÙ†', 'ØªÙŠÙ†Ùƒ', 'Ø«Ù…', 'Ø«Ù…Ø©', 'Ø­Ø§Ø´Ø§',\n",
        "                    'Ø­Ø¨Ø°Ø§', 'Ø­ØªÙ‰', 'Ø­ÙŠØ«', 'Ø­ÙŠØ«Ù…Ø§', 'Ø­ÙŠÙ†', 'Ø®Ù„Ø§', 'Ø¯ÙˆÙ†', 'Ø°Ø§', 'Ø°Ø§Øª', 'Ø°Ø§Ùƒ', 'Ø°Ø§Ù†', 'Ø°Ø§Ù†Ùƒ', 'Ø°Ù„Ùƒ', 'Ø°Ù„ÙƒÙ…', 'Ø°Ù„ÙƒÙ…Ø§', 'Ø°Ù„ÙƒÙ†', 'Ø°Ù‡', 'Ø°Ùˆ', 'Ø°ÙˆØ§', 'Ø°ÙˆØ§ØªØ§', 'Ø°ÙˆØ§ØªÙŠ', 'Ø°ÙŠ', 'Ø°ÙŠÙ†', \n",
        "                    'Ø°ÙŠÙ†Ùƒ', 'Ø±ÙŠØ«', 'Ø³ÙˆÙ', 'Ø³ÙˆÙ‰', 'Ø´ØªØ§Ù†', 'Ø¹Ø¯Ø§', 'Ø¹Ø³Ù‰', 'Ø¹Ù„', 'Ø¹Ù„Ù‰', 'Ø¹Ù„ÙŠÙƒ', 'Ø¹Ù„ÙŠÙ‡', 'Ø¹Ù…Ø§', 'Ø¹Ù†', 'Ø¹Ù†Ø¯', 'ØºÙŠØ±', 'ÙØ¥Ø°Ø§', 'ÙØ¥Ù†', 'ÙÙ„Ø§', 'ÙÙ…Ù†', 'ÙÙŠ', 'ÙÙŠÙ…', 'ÙÙŠÙ…Ø§', 'ÙÙŠÙ‡', \n",
        "                    'ÙÙŠÙ‡Ø§', 'Ù‚Ø¯', 'ÙƒØ£Ù†', 'ÙƒØ£Ù†Ù…Ø§', 'ÙƒØ£ÙŠ', 'ÙƒØ£ÙŠÙ†', 'ÙƒØ°Ø§', 'ÙƒØ°Ù„Ùƒ', 'ÙƒÙ„', 'ÙƒÙ„Ø§', 'ÙƒÙ„Ø§Ù‡Ù…Ø§', 'ÙƒÙ„ØªØ§', 'ÙƒÙ„Ù…Ø§', 'ÙƒÙ„ÙŠÙƒÙ…Ø§', 'ÙƒÙ„ÙŠÙ‡Ù…Ø§', 'ÙƒÙ…', 'ÙƒÙ…', 'ÙƒÙ…Ø§', 'ÙƒÙŠ', 'ÙƒÙŠØª', 'ÙƒÙŠÙ', 'ÙƒÙŠÙÙ…Ø§', 'Ù„Ø§',\n",
        "                    'Ù„Ø§Ø³ÙŠÙ…Ø§', 'Ù„Ø¯Ù‰', 'Ù„Ø³Øª', 'Ù„Ø³ØªÙ…', 'Ù„Ø³ØªÙ…Ø§', 'Ù„Ø³ØªÙ†', 'Ù„Ø³Ù†', 'Ù„Ø³Ù†Ø§', 'Ù„Ø¹Ù„', 'Ù„Ùƒ', 'Ù„ÙƒÙ…', 'Ù„ÙƒÙ…Ø§', 'Ù„ÙƒÙ†', 'Ù„ÙƒÙ†Ù…Ø§', 'Ù„ÙƒÙŠ', 'Ù„ÙƒÙŠÙ„Ø§', 'Ù„Ù…', 'Ù„Ù…Ø§', 'Ù„Ù†', 'Ù„Ù†Ø§', 'Ù„Ù‡', 'Ù„Ù‡Ø§', 'Ù„Ù‡Ù…', 'Ù„Ù‡Ù…Ø§', \n",
        "                    'Ù„Ù‡Ù†', 'Ù„Ùˆ', 'Ù„ÙˆÙ„Ø§', 'Ù„ÙˆÙ…Ø§', 'Ù„ÙŠ', 'Ù„Ø¦Ù†', 'Ù„ÙŠØª', 'Ù„ÙŠØ³', 'Ù„ÙŠØ³Ø§', 'Ù„ÙŠØ³Øª', 'Ù„ÙŠØ³ØªØ§', 'Ù„ÙŠØ³ÙˆØ§', 'Ù…Ø§', 'Ù…Ø§Ø°Ø§', 'Ù…ØªÙ‰', 'Ù…Ø°', 'Ù…Ø¹', 'Ù…Ù…Ø§', 'Ù…Ù…Ù†', 'Ù…Ù†', 'Ù…Ù†Ù‡', 'Ù…Ù†Ù‡Ø§', 'Ù…Ù†Ø°', 'Ù…Ù‡', 'Ù…Ù‡Ù…Ø§',\n",
        "                    'Ù†Ø­Ù†', 'Ù†Ø­Ùˆ', 'Ù†Ø¹Ù…', 'Ù‡Ø§', 'Ù‡Ø§ØªØ§Ù†', 'Ù‡Ø§ØªÙ‡', 'Ù‡Ø§ØªÙŠ', 'Ù‡Ø§ØªÙŠÙ†', 'Ù‡Ø§Ùƒ', 'Ù‡Ø§Ù‡Ù†Ø§', 'Ù‡Ø°Ø§', 'Ù‡Ø°Ø§Ù†', 'Ù‡Ø°Ù‡', 'Ù‡Ø°ÙŠ', 'Ù‡Ø°ÙŠÙ†', 'Ù‡ÙƒØ°Ø§', 'Ù‡Ù„', 'Ù‡Ù„Ø§', 'Ù‡Ù…', 'Ù‡Ù…Ø§', 'Ù‡Ù†', 'Ù‡Ù†Ø§', 'Ù‡Ù†Ø§Ùƒ', 'Ù‡Ù†Ø§Ù„Ùƒ',\n",
        "                    'Ù‡Ùˆ', 'Ù‡Ø¤Ù„Ø§Ø¡', 'Ù‡ÙŠ', 'Ù‡ÙŠØ§', 'Ù‡ÙŠØª', 'Ù‡ÙŠÙ‡Ø§Øª', 'ÙˆØ§Ù„Ø°ÙŠ', 'ÙˆØ§Ù„Ø°ÙŠÙ†', 'ÙˆØ¥Ø°', 'ÙˆØ¥Ø°Ø§', 'ÙˆØ¥Ù†', 'ÙˆÙ„Ø§', 'ÙˆÙ„ÙƒÙ†', 'ÙˆÙ„Ùˆ', 'ÙˆÙ…Ø§', 'ÙˆÙ…Ù†', 'ÙˆÙ‡Ùˆ', 'ÙŠØ§', 'ÙŠÙ…Ø§Ù„Ù‰', 'Ù‚Ù„', 'ÙƒØ«Ø±', 'Ø£Ù„ÙŠ', 'Ù„ÙŠÙƒ', 'Ù„ÙŠÙƒÙ…', \n",
        "                    'Ù†ØªÙŠØ§', 'Ù†ØªÙˆÙ…Ø§', 'Ø±Ø§Ù†Ø§' ,'Ø¨ØµØ­', 'Ù‡ÙˆÙ…Ø§', 'ÙˆÙŠÙ†', 'Ø£Ù…Ø¨Ø¹Ø¯', 'Ø£ÙˆÙ…Ø¨Ø¹Ø¯', 'Ø´ÙˆÙŠØ©', 'Ø´ÙˆÙŠØ§', 'ÙˆØ´', 'ÙˆØ§Ø´', 'Ø¨ÙˆØ´', 'Ø¨ÙˆØ§Ø´','Ùˆ'}\n",
        "def remove_stop_words(text):\n",
        "    zen = TextBlob(text)\n",
        "    words = zen.words\n",
        "    return \" \".join([w for w in words if not w in stops and not w in stop_word_comp and len(w) >= 2])\n",
        "\n",
        "\n",
        "def all_togther(text):\n",
        "  text = keep_only_arabic(text)\n",
        "  text = remove_empty_lines(text)\n",
        "  text = remove_emoji(text)\n",
        "  text = normalizeArabic(text)\n",
        "  text = char_rmvl(text)\n",
        "  text = text.lower()\n",
        "  text = remove_stop_words(text)\n",
        "  text = ISRIStemmer().suf32(text)\n",
        "  return text\n",
        "\n",
        "\n",
        "df['comments'] = df['comments'].apply(lambda x:all_togther(x))\n",
        "nan_value = float(\"NaN\")\n",
        "df.replace(\"\", nan_value, inplace=True)\n",
        "df.dropna(subset = [\"comments\"], inplace=True)\n",
        "#print(remove_emoji('ğŸŒ.Ø­ØªÙŠ Ø§Ù„ÙƒÙˆÙŠØª Ø¹Ø·Ù„Ø© Ø±Ø³Ù…ÙŠÙ‡ ğŸŒ· Ø§Ù„Ù…ÙˆÙ„Ø¯ ğŸ‘‘â¤ğŸ‘‘ğŸ¤²ğŸ¤²ğŸ‡²ğŸ‡¦ğŸ‡²ğŸ‡¦ğŸ‡²ğŸ‡¦ğŸ‡²ğŸ‡¦Ø§Ù„Ù†Ø¨ÙˆÙŠ âœŒ Ù…Ø§Ø°Ø§âŒ¨â†ªâ€¢â±â³â°â²#âƒ£ âª ğŸ“ºÙŠÙ‚ØµØ¯ ğŸ’©ØµØ·ÙŠÙ â™¥ï¸ğŸ¤²  â¦ÙØ±ÙƒØ§Ø´Ù‡?FakioğŸ‘‘ğŸ‘‘+ğŸ‡®ğŸ‡±ğŸ‡®ğŸ‡±ğŸªğŸªğŸªğŸªğŸªğŸªğŸªğŸ‘‰')) #we didn't catch some emoji\n",
        "#print(normalizeArabic('Ù…ÙØ§Ø°ÙØ§Ø§ÙØªÙØ®ÙØ°Ù Ù…ÙÙ†Ù› Ø§ÙØ¬Ù›Ø±ÙØ±Ø±Ø±Ø±Ø±Ø§Ø¡ÙØªÙ›ØŸ')) #done \n",
        "#print(char_rmvl('Ù…ÙØ§Ø°ÙØ§Ø§ÙØªÙØ®ÙØ°Ù05550 #$%&()*+,ØŒ-./:;<=>ØŸ?@[\\]^_`{|}Ù…ÙÙ†Ù› Ø§ÙØ¬Ù›Ø±ÙØ±Ø±Ø±Ø±Ø±Ø§Ø¡ÙØªÙ›  ğŸ‘‘â¤ğŸ‘‘ğŸ¤²ğŸ¤²ğŸ‡²ğŸ‡¦ğŸ‡²ğŸ‡¦ğŸ‡²ğŸ‡¦ğŸ‡²ğŸ‡¦Ø§Ù„Ù†Ø¨ÙˆÙŠ âœŒ Ù…Ø§Ø°Ø§âŒ¨â†ªâ€¢â±â³â°â²#âƒ£ âª ğŸ“ºØŸ')) #done \n",
        "#print(remove_stop_words('Ø¯ÙŠØ± Ø­Ù„ Ø±Ø§Ù†Ø§ ÙƒØ±Ù‡Ù†Ø§ Ù…Ù† Ø§Ù„ÙˆØ¹ÙˆØ¯ Ø§Ù„ÙƒØ§Ø°Ø¨Ø© ÙˆØ§Ù„Ù‡Ø¯Ø±Ø© Ø¨Ù„Ø§Ø´')) #done \n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: alphabet_detector in /usr/local/lib/python3.7/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.7/dist-packages (0.6.14)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pyarabic) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1wwBimpU8T9"
      },
      "source": [
        "downloading Fastext "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCZYYfkbCRTA",
        "outputId": "6dcf25a9-879a-4db3-9533-8ab1306ccfab"
      },
      "source": [
        "\n",
        "#Download FastText Embeddings (Vectors not the model)\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ar.300.vec.gz\n",
        "!gzip -q cc.ar.300.vec.gz -d .\n",
        "\n",
        "#%%\n",
        "\n",
        "#Loading FastText Embeddings.\n",
        "from tqdm import tqdm\n",
        "import codecs\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"Loading FastText Embeddings\"\"\"\n",
        "print(\"Loading FastText Embeddings...\")\n",
        "FastText = {}\n",
        "f = codecs.open('/content/cc.ar.300.vec', encoding='utf-8')\n",
        "for line in tqdm(f):\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    FastText[word] = coefs\n",
        "f.close()\n",
        "\n",
        "#%%\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-08 15:47:36--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ar.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1272365870 (1.2G) [binary/octet-stream]\n",
            "Saving to: â€˜cc.ar.300.vec.gzâ€™\n",
            "\n",
            "cc.ar.300.vec.gz    100%[===================>]   1.18G  21.8MB/s    in 56s     \n",
            "\n",
            "2021-08-08 15:48:34 (21.5 MB/s) - â€˜cc.ar.300.vec.gzâ€™ saved [1272365870/1272365870]\n",
            "\n",
            "gzip: cc.ar.300.vec already exists; do you wish to overwrite (y or n)? y\n",
            "y\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "934it [00:00, 9333.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading FastText Embeddings...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000001it [03:46, 8843.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4P-Ke9kVCq2"
      },
      "source": [
        "fasttext_embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsuISwnSFRIS",
        "outputId": "8427cad8-e5e3-4d1d-ad55-9efea019136d"
      },
      "source": [
        "\n",
        "#Tokenization and Sentence Encoding\n",
        "\n",
        "X_train_1 = df['comments']\n",
        "\n",
        "#Input_ids and Attention masks.\n",
        "X_train = []\n",
        "\n",
        "#Creating Embedding Matrix for fasttext embeddings\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing import text\n",
        "tokenizer = text.Tokenizer(num_words=30000)\n",
        "tokenizer.fit_on_texts(X_train_1)\n",
        "\n",
        "train_texts = tokenizer.texts_to_sequences(X_train_1)\n",
        "X_train_ft = sequence.pad_sequences(train_texts, maxlen=128)\n",
        "\n",
        "print(\"Dictionary: \"+str(len(tokenizer.word_index)))\n",
        "embedding_matrix = np.zeros((30000, 300))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index > 30000 - 1:\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = FastText.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary: 39137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipK_NL4CVHHj"
      },
      "source": [
        "spliting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwhuHHa-GoBC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = df['hate '].map({'yes':1,'no':0 })\n",
        "##Splitting fastText encoded data for training DL algorithms based on Bi-LSTMs and Bi-GRUs.\n",
        "## train data ==> (X_train_ft, y_train)\n",
        "## Validation data ==> (X_val_ft, y_val)\n",
        "## Test data ==> (X_test_ft, y_test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_ft,y)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6yfNgohVJZb"
      },
      "source": [
        "fasttext with LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqH5UU9rHiHL",
        "outputId": "8b0e3097-ddd0-4640-c073-e2401f03552a"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "model = Sequential()\n",
        "## embeddings layer (fasttext embedding 300 dimension size)\n",
        "model.add(Embedding(30000, 300, input_length=128, weights=[embedding_matrix], trainable=True))\n",
        "\n",
        "model.add(LSTM(units=128))\n",
        "#model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(X_train, y_train, validation_split=0.3, epochs=4)\n",
        "\n",
        "y_pred = (model.predict(X_test) >= 0.5)\n",
        "\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (None, 128, 300)          9000000   \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 128)               219648    \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 9,219,777\n",
            "Trainable params: 9,219,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "152/152 [==============================] - 65s 412ms/step - loss: 0.6485 - acc: 0.6307 - val_loss: 0.6112 - val_acc: 0.6684\n",
            "Epoch 2/4\n",
            "152/152 [==============================] - 62s 409ms/step - loss: 0.3427 - acc: 0.8566 - val_loss: 0.7921 - val_acc: 0.6646\n",
            "Epoch 3/4\n",
            "152/152 [==============================] - 62s 410ms/step - loss: 0.1109 - acc: 0.9694 - val_loss: 0.9019 - val_acc: 0.6279\n",
            "Epoch 4/4\n",
            "152/152 [==============================] - 62s 409ms/step - loss: 0.0397 - acc: 0.9907 - val_loss: 1.1900 - val_acc: 0.6414\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.60      0.61      1036\n",
            "           1       0.68      0.69      0.68      1266\n",
            "\n",
            "    accuracy                           0.65      2302\n",
            "   macro avg       0.65      0.65      0.65      2302\n",
            "weighted avg       0.65      0.65      0.65      2302\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls2v8kj7VO-P"
      },
      "source": [
        "fasttext with GRU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn-7s_1qTvdD",
        "outputId": "940b735c-3061-415c-ad39-50ab3bdc5fcd"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "model = Sequential()\n",
        "## embeddings layer (fasttext embedding 300 dimension size)\n",
        "model.add(Embedding(30000, 300, input_length=128, weights=[embedding_matrix], trainable=True))\n",
        "\n",
        "model.add(GRU(units=128))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(X_train, y_train, validation_split=0.3, epochs=4)\n",
        "\n",
        "y_pred = (model.predict(X_test) >= 0.5)\n",
        "\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_15 (Embedding)     (None, 128, 300)          9000000   \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 128)               165120    \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 9,165,249\n",
            "Trainable params: 9,165,249\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "152/152 [==============================] - 55s 353ms/step - loss: 0.6491 - acc: 0.6098 - val_loss: 0.6192 - val_acc: 0.6583\n",
            "Epoch 2/4\n",
            "152/152 [==============================] - 47s 311ms/step - loss: 0.3388 - acc: 0.8628 - val_loss: 0.7513 - val_acc: 0.6641\n",
            "Epoch 3/4\n",
            "152/152 [==============================] - 48s 313ms/step - loss: 0.0689 - acc: 0.9789 - val_loss: 1.1611 - val_acc: 0.6602\n",
            "Epoch 4/4\n",
            "152/152 [==============================] - 48s 314ms/step - loss: 0.0290 - acc: 0.9952 - val_loss: 1.3031 - val_acc: 0.6472\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.64      0.63      1036\n",
            "           1       0.69      0.67      0.68      1266\n",
            "\n",
            "    accuracy                           0.66      2302\n",
            "   macro avg       0.65      0.66      0.65      2302\n",
            "weighted avg       0.66      0.66      0.66      2302\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0DVbL6IVSWD"
      },
      "source": [
        "fasttext with Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR298hw6OkI_",
        "outputId": "a67be8a9-3b80-4693-ff16-3931b1e23606"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.__version__\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import warnings\n",
        "\n",
        "model = Sequential()\n",
        "model.add(tf.keras.layers.Embedding(30000, 300, input_length=128, weights=[embedding_matrix], trainable=True))\n",
        "\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(X_train, y_train, validation_split=0.3, epochs=4)\n",
        "\n",
        "y_pred = (model.predict(X_test) >= 0.5)\n",
        "\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     (None, 128, 300)          9000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 256)               439296    \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 9,439,553\n",
            "Trainable params: 9,439,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "152/152 [==============================] - 109s 695ms/step - loss: 0.6592 - binary_accuracy: 0.6063 - val_loss: 0.6057 - val_binary_accuracy: 0.6684\n",
            "Epoch 2/4\n",
            "152/152 [==============================] - 106s 695ms/step - loss: 0.3557 - binary_accuracy: 0.8546 - val_loss: 0.6842 - val_binary_accuracy: 0.6549\n",
            "Epoch 3/4\n",
            "152/152 [==============================] - 107s 701ms/step - loss: 0.0852 - binary_accuracy: 0.9743 - val_loss: 0.9174 - val_binary_accuracy: 0.6511\n",
            "Epoch 4/4\n",
            "152/152 [==============================] - 107s 702ms/step - loss: 0.0295 - binary_accuracy: 0.9942 - val_loss: 1.1894 - val_binary_accuracy: 0.6472\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.57      0.60      1036\n",
            "           1       0.67      0.73      0.70      1266\n",
            "\n",
            "    accuracy                           0.66      2302\n",
            "   macro avg       0.65      0.65      0.65      2302\n",
            "weighted avg       0.65      0.66      0.65      2302\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_iQvwVtVXXt"
      },
      "source": [
        "CNN with fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "id": "n5uovtlAONs-",
        "outputId": "2f92cd2f-022d-464f-b4fd-bf713c77d106"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "model = Sequential()\n",
        "## embeddings layer (fasttext embedding 300 dimension size)\n",
        "model.add(Embedding(30000, 300, input_length=128, weights=[embedding_matrix], trainable=True))\n",
        "model.add(Conv1D(activation='relu', filters=4, kernel_size=4))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(MaxPool1D())\n",
        "\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(X_train, y_train, validation_split=0.3, epochs=4)\n",
        "\n",
        "y_pred = (model.predict(X_test) >= 0.5)\n",
        "\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_17 (Embedding)     (None, 128, 300)          9000000   \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 125, 4)            4804      \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 125, 4)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 62, 4)             0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 62, 1)             5         \n",
            "=================================================================\n",
            "Total params: 9,004,809\n",
            "Trainable params: 9,004,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "152/152 [==============================] - 21s 131ms/step - loss: 0.6871 - acc: 0.5585 - val_loss: 0.6852 - val_acc: 0.5631\n",
            "Epoch 2/4\n",
            "152/152 [==============================] - 19s 128ms/step - loss: 0.6800 - acc: 0.5710 - val_loss: 0.6828 - val_acc: 0.5664\n",
            "Epoch 3/4\n",
            "152/152 [==============================] - 19s 126ms/step - loss: 0.6667 - acc: 0.5810 - val_loss: 0.6879 - val_acc: 0.5654\n",
            "Epoch 4/4\n",
            "152/152 [==============================] - 19s 127ms/step - loss: 0.6561 - acc: 0.5859 - val_loss: 0.6894 - val_acc: 0.5650\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-705e56522898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and unknown targets"
          ]
        }
      ]
    }
  ]
}